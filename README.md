Walmart Weekly Sales Forecasting üìà
ISA 444 Final ProjectAuthors: Daniel Woodward & Olivia Pisano
üìã Project Overview
This project focuses on forecasting weekly sales for Walmart stores using a variety of time-series forecasting techniques.
By leveraging historical sales data and exogenous variables (such as holidays, temperature, fuel price, and CPI), we aimed to predict future sales performance to assist in inventory management and business planning.The project implements a "Tournament" style modeling approach, comparing Statistical Models, Machine Learning Models, Deep Learning Models, and Foundation Models (TimeGPT) to determine the most accurate forecasting method.
‚ö†Ô∏è Important: Environment Requirements
This notebook is specifically designed to run in a Linux-based environment, such as Google Colab.Recommended Platform: Google ColabIncompatibility: This code cannot be run locally on Windows (e.g., VS Code) without significant modification.Reasoning: The "Auto" models used in this project (specifically AutoNBEATS and AutoNHITS from the neuralforecast library) utilize the Ray framework for hyperparameter tuning. The specific implementation of Ray and multiprocessing used in this codebase requires a Linux/Unix backend to execute correctly. Running this on a standard Windows backend will result in multiprocessing errors.
üîê Security Note: API Key Usage
You may notice a hardcoded API key for TimeGPT (Nixtla) within the notebook.Context: We understand that hardcoding credentials is generally considered poor security practice.Exception: This key is provided strictly for academic purposes under a temporary, school-based subscription managed by our professor for the ISA 444 course.Validity: The key is transient and will be invalidated/rotated shortly after the grading period. It is included here solely to ensure the code is reproducible for grading.
üõ†Ô∏è Tech Stack & Libraries
This project utilizes the Python ecosystem for time-series analysis, specifically the Nixtla suite of libraries:Data Manipulation: pandas, numpyVisualization: matplotlib, seabornStatistical Models: statsforecast (AutoARIMA, AutoETS, Naive, SeasonalNaive)Machine Learning: mlforecast (LightGBM)Deep Learning: neuralforecast (AutoNBEATS, AutoNHITS)Foundation Models: nixtla (TimeGPT)
üìÇ Data Preparation
The dataset is based on the Walmart Recruiting - Store Sales Forecasting challenge.Data Merging: We merged Sales data with Store metadata and Economic features (CPI, Unemployment, Fuel Price).Preprocessing:Handled missing values in Markdown columns.Interpolated missing economic indicators using forward/backward fill.Created unique identifiers (unique_id) based on Store and Department combinations.Downsampling: To ensure computational efficiency for the final project, we focused on the Top 20 high-volume series based on total sales.
‚öôÔ∏è Modeling Pipeline
We implemented a robust pipeline to train and evaluate models using Cross-Validation (5 windows, 4-week horizon).CategoryModels UsedBaselinesNaive, Seasonal NaiveStatisticalAutoARIMA, AutoETSMachine LearningLightGBM (Gradient Boosting) with lag featuresDeep LearningAutoNBEATS, AutoNHITS (Run on Linux/Colab)GenAI / LLMTimeGPT (via API)üìä Key Findings & ResultsAfter running the evaluation pipeline, we compared the models based on RMSE (Root Mean Squared Error) and MAPE (Mean Absolute Percentage Error).Final Performance MetricsThe AutoNBEATS (Neural Hierarchical Interpolation) model proved to be the superior performer for this dataset.
ModelMAERMSEMAPEAutoNBEATS (Winner) üèÜ7,1079,7825.17%AutoETS7,84510,6625.67%AutoARIMA7,41811,4115.47%AutoNHITS9,42512,8976.77%TimeGPT9,66913,3087.12%LightGBM10,39914,6457.72%SeasonalNaive11,29815,8618.18%Leaderboard (Wins per Series)Out of the 20 distinct store/department series analyzed:AutoNBEATS: Won 9 series.AutoARIMA: Won 4 series.SeasonalNaive: Won 3 series.AutoETS: Won 2 series.AutoNHITS & TimeGPT: Won 1 series each.Visual AnalysisAs seen in the sample forecast for unique_id: 10_72:The Actual Sales (Black line) show significant volatility.AutoNBEATS (Brown line) and AutoARIMA (Red line) tracked the peaks and troughs of the validation set most closely.LightGBM and TimeGPT struggled with some specific seasonal spikes in this subset, leading to higher error rates globally.üß† Lessons LearnedThrough this project, we learned several key insights regarding Time Series forecasting:Deep Learning Superiority: For high-volume, volatile retail data, Deep Learning models (specifically N-BEATS) outperformed traditional statistical methods and standard machine learning trees.The "No Free Lunch" Theorem: While AutoNBEATS was the global winner, it did not win every series. Simple baselines like SeasonalNaive still performed best on 3 specific series, proving that complex models are not always the answer for every individual trend.Complexity of Foundation Models: Implementing TimeGPT provided insight into using APIs for forecasting. While powerful, it requires careful handling of rate limits and timeouts (as experienced during our initial runs) compared to local models.Feature Importance: The inclusion of exogenous variables (Holidays, CPI) was critical. Models that could effectively ingest these external factors generally performed better during peak holiday weeks.üíª How to Run This CodeDownload the Files: Ensure you have the .ipynb file and the dataset CSVs (train.csv, test.csv, features.csv, stores.csv).Upload to Google Colab: Go to colab.research.google.com and upload the notebook.Upload Data: Upload the CSV files to the Colab session storage.Install Libraries: Run the first cell to install the required Linux-compatible libraries:Python!pip install pandas numpy matplotlib seaborn statsforecast mlforecast neuralforecast lightgbm nixtla
Run All Cells: Execute the notebook cells in order.
