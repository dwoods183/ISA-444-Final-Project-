{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ISA 444 Final Project\n",
        "# Daniel Woodward, Olivia Pisano"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuU6bC1D0qzq",
        "outputId": "8da3eb02-ecb6-4c2c-f9c9-59d81de9e133"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.6)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.10.7)\n",
            "Requirement already satisfied: seaborn in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.2)\n",
            "Requirement already satisfied: statsforecast in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.0.3)\n",
            "Requirement already satisfied: mlforecast in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.2)\n",
            "Requirement already satisfied: neuralforecast in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.1.2)\n",
            "Requirement already satisfied: lightgbm in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.6.0)\n",
            "Requirement already satisfied: nixtla in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\woodw\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\woodw\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (12.0.0)\n",
            "Requirement already satisfied: pyparsing>=3 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: cloudpickle in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsforecast) (3.1.2)\n",
            "Requirement already satisfied: coreforecast>=0.0.12 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsforecast) (0.0.16)\n",
            "Requirement already satisfied: numba>=0.55.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsforecast) (0.61.2)\n",
            "Requirement already satisfied: scipy<1.16.0,>=1.7.3 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsforecast) (1.15.3)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsforecast) (0.14.5)\n",
            "Requirement already satisfied: tqdm in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsforecast) (4.67.1)\n",
            "Requirement already satisfied: fugue>=0.8.1 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsforecast) (0.9.3)\n",
            "Requirement already satisfied: utilsforecast>=0.1.4 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsforecast) (0.2.15)\n",
            "Requirement already satisfied: threadpoolctl>=3 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsforecast) (3.6.0)\n",
            "Requirement already satisfied: fsspec in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlforecast) (2025.12.0)\n",
            "Requirement already satisfied: optuna in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlforecast) (4.6.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlforecast) (1.7.2)\n",
            "Requirement already satisfied: torch>=2.4.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from neuralforecast) (2.9.1)\n",
            "Requirement already satisfied: pytorch-lightning>=2.0.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from neuralforecast) (2.6.0)\n",
            "Requirement already satisfied: ray[tune]>=2.2.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from neuralforecast) (2.52.1)\n",
            "Requirement already satisfied: annotated-types in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nixtla) (0.7.0)\n",
            "Requirement already satisfied: httpx[zstd] in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nixtla) (0.28.1)\n",
            "Requirement already satisfied: orjson in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nixtla) (3.11.4)\n",
            "Requirement already satisfied: pydantic>=1.10 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nixtla) (2.12.3)\n",
            "Requirement already satisfied: tenacity in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nixtla) (9.1.2)\n",
            "Requirement already satisfied: triad>=1.0.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fugue>=0.8.1->statsforecast) (1.0.0)\n",
            "Requirement already satisfied: adagio>=0.2.6 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fugue>=0.8.1->statsforecast) (0.2.6)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from numba>=0.55.0->statsforecast) (0.44.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=1.10->nixtla) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=1.10->nixtla) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=1.10->nixtla) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\woodw\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: PyYAML>5.4 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-lightning>=2.0.0->neuralforecast) (6.0.1)\n",
            "Requirement already satisfied: torchmetrics>0.7.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-lightning>=2.0.0->neuralforecast) (1.8.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-lightning>=2.0.0->neuralforecast) (0.15.2)\n",
            "Requirement already satisfied: click!=8.3.*,>=7.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ray[tune]>=2.2.0->neuralforecast) (8.2.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ray[tune]>=2.2.0->neuralforecast) (3.20.0)\n",
            "Requirement already satisfied: jsonschema in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ray[tune]>=2.2.0->neuralforecast) (4.25.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ray[tune]>=2.2.0->neuralforecast) (1.0.3)\n",
            "Requirement already satisfied: protobuf>=3.20.3 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ray[tune]>=2.2.0->neuralforecast) (6.33.1)\n",
            "Requirement already satisfied: requests in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ray[tune]>=2.2.0->neuralforecast) (2.32.5)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ray[tune]>=2.2.0->neuralforecast) (2.6.4)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ray[tune]>=2.2.0->neuralforecast) (22.0.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsmodels>=0.13.2->statsforecast) (1.0.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.4.0->neuralforecast) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.4.0->neuralforecast) (3.6)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.4.0->neuralforecast) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.4.0->neuralforecast) (80.9.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\woodw\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->statsforecast) (0.4.6)\n",
            "Requirement already satisfied: narwhals>=2.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from utilsforecast>=0.1.4->statsforecast) (2.10.2)\n",
            "Requirement already satisfied: anyio in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx[zstd]->nixtla) (4.11.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx[zstd]->nixtla) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx[zstd]->nixtla) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx[zstd]->nixtla) (3.11)\n",
            "Requirement already satisfied: zstandard>=0.18.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx[zstd]->nixtla) (0.25.0)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx[zstd]->nixtla) (0.16.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna->mlforecast) (1.17.2)\n",
            "Requirement already satisfied: colorlog in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna->mlforecast) (6.10.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna->mlforecast) (2.0.44)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->mlforecast) (1.5.2)\n",
            "Requirement already satisfied: Mako in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from alembic>=1.5.0->optuna->mlforecast) (1.3.10)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec->mlforecast) (3.13.0)\n",
            "Requirement already satisfied: greenlet>=1 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna->mlforecast) (3.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch>=2.4.0->neuralforecast) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio->httpx[zstd]->nixtla) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=2.4.0->neuralforecast) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema->ray[tune]>=2.2.0->neuralforecast) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema->ray[tune]>=2.2.0->neuralforecast) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema->ray[tune]>=2.2.0->neuralforecast) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema->ray[tune]>=2.2.0->neuralforecast) (0.30.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->ray[tune]>=2.2.0->neuralforecast) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->ray[tune]>=2.2.0->neuralforecast) (1.26.20)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->mlforecast) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->mlforecast) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->mlforecast) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->mlforecast) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->mlforecast) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\woodw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->mlforecast) (1.22.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 1. Installation & Imports\n",
        "# ==========================================\n",
        "!pip install pandas numpy matplotlib seaborn statsforecast mlforecast neuralforecast lightgbm nixtla\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# StatsForecast\n",
        "from statsforecast import StatsForecast\n",
        "from statsforecast.models import Naive, SeasonalNaive, AutoETS, AutoARIMA\n",
        "\n",
        "# MLForecast\n",
        "from mlforecast import MLForecast\n",
        "from mlforecast.target_transforms import Differences\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# NeuralForecast\n",
        "from neuralforecast import NeuralForecast\n",
        "from neuralforecast.auto import AutoNBEATS, AutoNHITS\n",
        "\n",
        "# TimeGPT\n",
        "from nixtla import NixtlaClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3YjoYo83cd9",
        "outputId": "92193add-1ecc-4d59-f971-48a950bf97b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "\n",
            "--- DIAGNOSTIC CHECK ---\n",
            "Total Sum of 'y' (Sales): 402,568,823.37\n",
            "STATUS: Data verified. Sales column 'y' is populated.\n",
            "------------------------\n",
            "\n",
            "Data Prepared. Modeling 20 series.\n",
            "\n",
            "Success! First 5 rows of testing data:\n",
            "   Store  Dept  IsHoliday  Type    Size  Temperature  Fuel_Price  MarkDown1  \\\n",
            "0      1    92          0     1  151315        55.32       3.386    6766.44   \n",
            "1      1    92          0     1  151315        61.24       3.314   11421.32   \n",
            "2      1    92          0     1  151315        52.92       3.252    9696.28   \n",
            "3      1    92          1     1  151315        56.23       3.211     883.59   \n",
            "4      1    92          0     1  151315        52.34       3.207    2460.03   \n",
            "\n",
            "   MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment  \\\n",
            "0    5147.70      50.82    3639.90    2737.42  223.462779         6.573   \n",
            "1    3370.89      40.28    4646.79    6154.16  223.481307         6.573   \n",
            "2     292.10     103.78    1133.15    6612.69  223.512911         6.573   \n",
            "3       4.17   74910.32     209.91     303.32  223.561947         6.573   \n",
            "4       0.00    3838.35     150.57    6966.34  223.610984         6.573   \n",
            "\n",
            "          ds unique_id  \n",
            "0 2012-11-02      1_92  \n",
            "1 2012-11-09      1_92  \n",
            "2 2012-11-16      1_92  \n",
            "3 2012-11-23      1_92  \n",
            "4 2012-11-30      1_92  \n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 2. Data Loading & Preprocessing\n",
        "# ==========================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def load_and_prep_data():\n",
        "    print(\"Loading data...\")\n",
        "\n",
        "    # 1. Load CSVs\n",
        "    try:\n",
        "        train = pd.read_csv('train.csv')\n",
        "        features = pd.read_csv('features.csv')\n",
        "        test = pd.read_csv('test.csv')\n",
        "        stores = pd.read_csv('stores.csv')\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error finding file: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    # 2. Merge Dataframes\n",
        "    df = train.merge(stores, on='Store', how='left')\n",
        "    df = df.merge(features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "\n",
        "    # Merge Test data as well\n",
        "    test_df = test.merge(stores, on='Store', how='left')\n",
        "    test_df = test_df.merge(features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "\n",
        "    # 3. Handle Missing Values\n",
        "\n",
        "    md_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "    df[md_cols] = df[md_cols].fillna(0)\n",
        "    test_df[md_cols] = test_df[md_cols].fillna(0)\n",
        "\n",
        "    # Economic Indicators: Interpolate (Fill forward, then backward)\n",
        "    exog_cols_to_fill = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "    for col in exog_cols_to_fill:\n",
        "        df[col] = df.groupby('Store')[col].transform(lambda x: x.ffill().bfill())\n",
        "        test_df[col] = test_df.groupby('Store')[col].transform(lambda x: x.ffill().bfill())\n",
        "\n",
        "    # 4. Formatting for Time Series Models\n",
        "    df['ds'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "    # DIAGNOSTIC 1: Check target before renaming\n",
        "    if 'Weekly_Sales' not in df.columns:\n",
        "        print(f\"CRITICAL ERROR: 'Weekly_Sales' column not found. Available columns: {df.columns.tolist()}\")\n",
        "        return None, None\n",
        "    # --------------------------------------------------\n",
        "\n",
        "    df = df.rename(columns={'Weekly_Sales': 'y'})\n",
        "    df['unique_id'] = df['Store'].astype(str) + '_' + df['Dept'].astype(str)\n",
        "\n",
        "    test_df['ds'] = pd.to_datetime(test_df['Date'])\n",
        "    test_df['unique_id'] = test_df['Store'].astype(str) + '_' + test_df['Dept'].astype(str)\n",
        "\n",
        "    # 5. Downsample: Select Top 20 series by volume\n",
        "    top_series = df.groupby('unique_id')['y'].sum().nlargest(20).index\n",
        "    df_subset = df[df['unique_id'].isin(top_series)].reset_index(drop=True)\n",
        "    test_subset = test_df[test_df['unique_id'].isin(top_series)].reset_index(drop=True)\n",
        "\n",
        "    # 6. Feature Engineering (Encodings)\n",
        "    df_subset['IsHoliday'] = df_subset['IsHoliday'].astype(int)\n",
        "    df_subset['Type'] = df_subset['Type'].map({'A': 1, 'B': 2, 'C': 3})\n",
        "    test_subset['IsHoliday'] = test_subset['IsHoliday'].astype(int)\n",
        "    test_subset['Type'] = test_subset['Type'].map({'A': 1, 'B': 2, 'C': 3})\n",
        "\n",
        "    # Ensure continuous date ranges for each unique_id in training data\n",
        "    all_series_dfs_train = []\n",
        "    for uid in df_subset['unique_id'].unique():\n",
        "        series_df = df_subset[df_subset['unique_id'] == uid].copy()\n",
        "        min_ds = series_df['ds'].min()\n",
        "        max_ds = series_df['ds'].max()\n",
        "\n",
        "        # This aligns the generated timeline with the Walmart Friday dates\n",
        "        full_date_range = pd.date_range(start=min_ds, end=max_ds, freq='W-FRI')\n",
        "        # ----------------------------------------------------\n",
        "\n",
        "        full_series_df = pd.DataFrame({'ds': full_date_range, 'unique_id': uid})\n",
        "\n",
        "        # Merge back with original series data\n",
        "        series_df = full_series_df.merge(series_df, on=['unique_id', 'ds'], how='left')\n",
        "        all_series_dfs_train.append(series_df)\n",
        "\n",
        "    df_subset = pd.concat(all_series_dfs_train).reset_index(drop=True)\n",
        "\n",
        "    # Final Safety Check: Ensure no NaNs remain in features used for ML\n",
        "    exogenous_ml_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "                         'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5',\n",
        "                         'IsHoliday', 'Type', 'Size']\n",
        "\n",
        "    # Fill any remaining NaNs (e.g., from reindexing or initial missing data) with 0\n",
        "    df_subset['y'] = df_subset['y'].fillna(0)\n",
        "    df_subset[exogenous_ml_cols] = df_subset[exogenous_ml_cols].fillna(0)\n",
        "    test_subset[exogenous_ml_cols] = test_subset[exogenous_ml_cols].fillna(0)\n",
        "\n",
        "    # Drop the original 'Date' column\n",
        "    df_subset = df_subset.drop(columns=['Date'], errors='ignore')\n",
        "    test_subset = test_subset.drop(columns=['Date'], errors='ignore')\n",
        "\n",
        "    #  DIAGNOSTIC 2: Verify Sales Sum\n",
        "    print(\"\\n--- DIAGNOSTIC CHECK ---\")\n",
        "    if 'y' in df_subset.columns:\n",
        "        total_y = df_subset['y'].sum()\n",
        "        print(f\"Total Sum of 'y' (Sales): {total_y:,.2f}\")\n",
        "\n",
        "        if total_y == 0:\n",
        "            print(\"CRITICAL ERROR: 'y' still contains only 0s.\")\n",
        "            return None, None\n",
        "        else:\n",
        "            print(\"STATUS: Data verified. Sales column 'y' is populated.\")\n",
        "    else:\n",
        "        print(\"CRITICAL ERROR: 'y' column disappeared.\")\n",
        "        return None, None\n",
        "    print(\"------------------------\\n\")\n",
        "    # ---------------------------------------\n",
        "\n",
        "    print(f\"Data Prepared. Modeling {df_subset['unique_id'].nunique()} series.\")\n",
        "    return df_subset, test_subset\n",
        "\n",
        "# EXECUTE THE FUNCTION\n",
        "train_subset, test_subset = load_and_prep_data()\n",
        "\n",
        "# Verify results\n",
        "if train_subset is not None and test_subset is not None:\n",
        "    print(\"\\nSuccess! First 5 rows of testing data:\")\n",
        "    print(test_subset.head())\n",
        "else:\n",
        "    print(\"Error: Data loading and preparation failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJIq0XcO340-",
        "outputId": "f6c04aa6-b48b-4e9d-abcc-25f88b8d07e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Running TimeGPT ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:nixtla.nixtla_client:Happy Forecasting! :)\n",
            "INFO:nixtla.nixtla_client:Validating inputs...\n",
            "INFO:nixtla.nixtla_client:Preprocessing dataframes...\n",
            "INFO:nixtla.nixtla_client:Querying model metadata...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Running TimeGPT Cross-Validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:nixtla.nixtla_client:Using future exogenous features: ['Store', 'Dept', 'IsHoliday', 'Type', 'Size', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment']\n",
            "INFO:nixtla.nixtla_client:Calling Cross Validation Endpoint...\n",
            "INFO:nixtla.nixtla_client:Validating inputs...\n",
            "INFO:nixtla.nixtla_client:Preprocessing dataframes...\n",
            "WARNING:nixtla.nixtla_client:The specified horizon \"h\" exceeds the model horizon, this may lead to less accurate forecasts. Please consider using a smaller horizon.\n",
            "INFO:nixtla.nixtla_client:Using future exogenous features: ['Store', 'Dept', 'IsHoliday', 'Type', 'Size', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment']\n",
            "INFO:nixtla.nixtla_client:Calling Forecast Endpoint...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Running TimeGPT Future Forecast (39 weeks)...\n",
            "\n",
            "Success! TimeGPT Cross-Validation Head:\n",
            "  unique_id         ds     cutoff          y     TimeGPT\n",
            "0     10_72 2012-06-15 2012-06-08  105499.39  115114.920\n",
            "1     10_72 2012-06-22 2012-06-08  107949.41  181358.220\n",
            "2     10_72 2012-06-29 2012-06-08   96579.10  133881.170\n",
            "3     10_72 2012-07-06 2012-06-08  100464.25  148005.270\n",
            "4     10_72 2012-07-13 2012-07-06   92923.05  113516.984\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 3. TimeGPT Pipeline\n",
        "# ==========================================\n",
        "from nixtla import NixtlaClient\n",
        "import pandas as pd\n",
        "\n",
        "def run_timegpt_pipeline(df_train, df_test, horizon, n_windows):\n",
        "    print(\"\\n--- Running TimeGPT ---\")\n",
        "\n",
        "    # 1. PASTE YOUR API KEY HERE\n",
        "    # ---------------------------------------------------------\n",
        "    my_api_key = 'nixak-eX92lSIatoWxvaVDNG4v7IFJQ5cva8FrMRaNuADzsiAlt007Tm8ejGc7VUU0MJEt2YZtsa2CmsfcbYPg'\n",
        "    # ---------------------------------------------------------\n",
        "\n",
        "    try:\n",
        "        # Initialize the client\n",
        "        nixtla_client = NixtlaClient(api_key=my_api_key)\n",
        "\n",
        "        # Validates the key before running heavy tasks\n",
        "        nixtla_client.validate_api_key()\n",
        "\n",
        "        # A. Cross-Validation\n",
        "        # We use 'W-FRI' because the data ends on Fridays\n",
        "        print(\"  Running TimeGPT Cross-Validation...\")\n",
        "        timegpt_cv = nixtla_client.cross_validation(\n",
        "            df=df_train,\n",
        "            h=horizon,\n",
        "            n_windows=n_windows,\n",
        "            step_size=horizon,\n",
        "            freq='W-FRI'\n",
        "        )\n",
        "\n",
        "        # B. Future Forecast\n",
        "        # Prepare future exogenous variables (Test set without the target 'y')\n",
        "        future_exog = df_test.drop(columns=['Date', 'Weekly_Sales', 'y'], errors='ignore')\n",
        "\n",
        "        # Calculate horizon based on test set length (should be 39 weeks for this dataset)\n",
        "        test_horizon = df_test.groupby('unique_id')['ds'].count().max()\n",
        "\n",
        "        print(f\"  Running TimeGPT Future Forecast ({test_horizon} weeks)...\")\n",
        "        timegpt_fcst = nixtla_client.forecast(\n",
        "            df=df_train,\n",
        "            h=test_horizon,\n",
        "            X_df=future_exog,\n",
        "            freq='W-FRI'\n",
        "        )\n",
        "\n",
        "        return timegpt_cv, timegpt_fcst\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error running TimeGPT: {e}\")\n",
        "        print(\"TIP: Check if your API key is pasted correctly inside the quotes.\")\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "# EXECUTE THE FUNCTION\n",
        "if 'train_subset' in globals() and 'test_subset' in globals():\n",
        "    # Run pipeline\n",
        "    tgpt_cv, tgpt_fcst = run_timegpt_pipeline(train_subset, test_subset, horizon=4, n_windows=5)\n",
        "\n",
        "    # Print results if successful\n",
        "    if not tgpt_cv.empty:\n",
        "        print(\"\\nSuccess! TimeGPT Cross-Validation Head:\")\n",
        "        print(tgpt_cv.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2bde3429839241c78b56644d387f00d8",
            "cc25f337b0c0499baa683f80fcaab583",
            "a112c8827a5d4fbca42d0e7eed3c73c7",
            "2209404beb6840a79cf0e4ca1640eadf",
            "fbc59f91b6df49cc9163c5cc6b3be16e",
            "79a1e0e5a458433985cc96d6332abc7c",
            "165bba3b67174d3a8e9cf02d0420a0d3",
            "776a083e13504f6b80bbf0908cec4d40"
          ]
        },
        "id": "NkYMg1Gy69Dz",
        "outputId": "6e475cc3-8537-487a-ef7d-fd6c7795d4f4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_train_tune pid=32564)\u001b[0m c:\\Users\\woodw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\integration\\pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
            "\u001b[36m(_train_tune pid=32564)\u001b[0m Seed set to 6\n",
            "\u001b[36m(_train_tune pid=32564)\u001b[0m GPU available: False, used: False\n",
            "\u001b[36m(_train_tune pid=32564)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(_train_tune pid=32564)\u001b[0m \n",
            "\u001b[36m(_train_tune pid=32564)\u001b[0m   | Name         | Type          | Params | Mode  | FLOPs\n",
            "\u001b[36m(_train_tune pid=32564)\u001b[0m ---------------------------------------------------------------\n",
            "\u001b[36m(_train_tune pid=32564)\u001b[0m 0 | loss         | MAE           | 0      | train | 0    \n",
            "\u001b[36m(_train_tune pid=32564)\u001b[0m 1 | padder_train | ConstantPad1d | 0      | train | 0    \n",
            "\u001b[36m(_train_tune pid=32564)\u001b[0m 2 | scaler       | TemporalNorm  | 0      | train | 0    \n",
            "\u001b[36m(_train_tune pid=32564)\u001b[0m 3 | blocks       | ModuleList    | 2.4 M  | train | 0    \n",
            "\u001b[36m(_train_tune pid=32564)\u001b[0m ---------------------------------------------------------------\n",
            "\u001b[36m(_train_tune pid=32564)\u001b[0m 2.4 M     Trainable params\n",
            "\u001b[36m(_train_tune pid=32564)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(_train_tune pid=32564)\u001b[0m 2.4 M     Total params\n",
            "\u001b[36m(_train_tune pid=32564)\u001b[0m 9.659     Total estimated model params size (MB)\n",
            "\u001b[36m(_train_tune pid=32564)\u001b[0m 34        Modules in train mode\n",
            "\u001b[36m(_train_tune pid=32564)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(_train_tune pid=32564)\u001b[0m 0         Total Flops\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.83e+16, train_loss_epoch=2.83e+16]        \n",
            "Epoch 2: 100%|██████████| 1/1 [00:00<00:00, 71.41it/s, v_num=0, train_loss_step=3.83e+15, train_loss_epoch=3.83e+15]\n",
            "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.83e+15, train_loss_epoch=3.83e+15]        \n",
            "Epoch 3: 100%|██████████| 1/1 [00:00<00:00, 62.46it/s, v_num=0, train_loss_step=5.6e+15, train_loss_epoch=5.6e+15]  \n",
            "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5.6e+15, train_loss_epoch=5.6e+15]        \n",
            "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.45e+13, train_loss_epoch=4.45e+13]        \n",
            "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=8.18e+14, train_loss_epoch=8.18e+14]        \n",
            "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.12e+16, train_loss_epoch=1.12e+16]        \n",
            "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.32e+15, train_loss_epoch=2.32e+15]        \n",
            "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.64e+16, train_loss_epoch=2.64e+16]        \n",
            "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.3e+16, train_loss_epoch=1.3e+16]           \n",
            "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.19e+15, train_loss_epoch=2.19e+15]        \n",
            "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.67e+14, train_loss_epoch=1.67e+14]        \n",
            "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.65e+13, train_loss_epoch=3.65e+13]         \n",
            "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=8e+12, train_loss_epoch=8e+12]              \n",
            "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.29e+13, train_loss_epoch=3.29e+13]        \n",
            "Epoch 58: 100%|██████████| 1/1 [00:00<00:00, 68.60it/s, v_num=0, train_loss_step=2.66e+13, train_loss_epoch=3.29e+13]\n",
            "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.66e+13, train_loss_epoch=2.66e+13]        \n",
            "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.14e+13, train_loss_epoch=1.14e+13]        \n",
            "Epoch 74: 100%|██████████| 1/1 [00:00<00:00, 71.30it/s, v_num=0, train_loss_step=8.78e+13, train_loss_epoch=8.78e+13]\n",
            "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=8.78e+13, train_loss_epoch=8.78e+13]        \n",
            "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.2e+14, train_loss_epoch=2.2e+14]          \n",
            "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.5e+13, train_loss_epoch=1.5e+13]          \n",
            "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=7.67e+13, train_loss_epoch=7.67e+13]        \n",
            "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 73.96it/s, v_num=0, train_loss_step=5.84e+13, train_loss_epoch=1.17e+14]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 648.67it/s]\u001b[A\n",
            "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.03e+14, train_loss_epoch=1.03e+14, valid_loss=6.64e+13]        \n",
            "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.65e+13, train_loss_epoch=4.65e+13, valid_loss=6.64e+13]        \n",
            "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.63e+13, train_loss_epoch=3.63e+13, valid_loss=6.64e+13]        \n",
            "Epoch 129:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=6.84e+13, train_loss_epoch=6.84e+13, valid_loss=6.64e+13]        \n",
            "Epoch 136: 100%|██████████| 1/1 [00:00<00:00, 76.82it/s, v_num=0, train_loss_step=9.04e+12, train_loss_epoch=5.41e+13, valid_loss=6.64e+13]\n",
            "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=9.04e+12, train_loss_epoch=9.04e+12, valid_loss=6.64e+13]        \n",
            "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.96e+12, train_loss_epoch=1.96e+12, valid_loss=6.64e+13]        \n",
            "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5.3e+12, train_loss_epoch=5.3e+12, valid_loss=6.64e+13]          \n",
            "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.68e+13, train_loss_epoch=3.68e+13, valid_loss=6.64e+13]        \n",
            "Epoch 160: 100%|██████████| 1/1 [00:00<00:00, 71.33it/s, v_num=0, train_loss_step=3.68e+13, train_loss_epoch=3.68e+13, valid_loss=6.64e+13]\n",
            "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.17e+13, train_loss_epoch=2.17e+13, valid_loss=6.64e+13]        \n",
            "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.24e+13, train_loss_epoch=2.24e+13, valid_loss=6.64e+13]         \n",
            "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=7.21e+12, train_loss_epoch=7.21e+12, valid_loss=6.64e+13]        \n",
            "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.96e+12, train_loss_epoch=4.96e+12, valid_loss=6.64e+13]        \n",
            "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.48e+12, train_loss_epoch=3.48e+12, valid_loss=6.64e+13]        \n",
            "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.63e+13, train_loss_epoch=1.63e+13, valid_loss=6.64e+13]        \n",
            "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 76.80it/s, v_num=0, train_loss_step=1.7e+12, train_loss_epoch=4.65e+12, valid_loss=6.64e+13] \n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 499.62it/s]\u001b[A\n",
            "Epoch 201:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=9.51e+12, train_loss_epoch=9.51e+12, valid_loss=9.54e+12]        \n",
            "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.4e+14, train_loss_epoch=4.4e+14, valid_loss=9.54e+12]          \n",
            "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.13e+13, train_loss_epoch=1.13e+13, valid_loss=9.54e+12]        \n",
            "Epoch 209:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.13e+13, train_loss_epoch=1.13e+13, valid_loss=9.54e+12]\n",
            "Epoch 217:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.4e+13, train_loss_epoch=4.4e+13, valid_loss=9.54e+12]          \n",
            "Epoch 225:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.74e+13, train_loss_epoch=2.74e+13, valid_loss=9.54e+12]        \n",
            "Epoch 233:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=6.68e+12, train_loss_epoch=6.68e+12, valid_loss=9.54e+12]        \n",
            "Epoch 240:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=7.3e+13, train_loss_epoch=7.3e+13, valid_loss=9.54e+12]          \n",
            "Epoch 241:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.57e+13, train_loss_epoch=2.57e+13, valid_loss=9.54e+12]        \n",
            "Epoch 249:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.53e+12, train_loss_epoch=3.53e+12, valid_loss=9.54e+12]         \n",
            "Epoch 257:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=9.88e+11, train_loss_epoch=9.88e+11, valid_loss=9.54e+12]        \n",
            "Epoch 265:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.32e+12, train_loss_epoch=3.32e+12, valid_loss=9.54e+12]        \n",
            "Epoch 273:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=9.41e+12, train_loss_epoch=9.41e+12, valid_loss=9.54e+12]         \n",
            "Epoch 274:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.17e+13, train_loss_epoch=3.17e+13, valid_loss=9.54e+12]        \n",
            "Epoch 281:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.3e+13, train_loss_epoch=2.3e+13, valid_loss=9.54e+12]          \n",
            "Epoch 289:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.19e+13, train_loss_epoch=1.19e+13, valid_loss=9.54e+12]        \n",
            "Epoch 297:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=6.45e+12, train_loss_epoch=6.45e+12, valid_loss=9.54e+12]        \n",
            "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 69.49it/s, v_num=0, train_loss_step=1.41e+13, train_loss_epoch=1.34e+13, valid_loss=9.54e+12]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 498.49it/s]\u001b[A\n",
            "Epoch 304:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1e+13, train_loss_epoch=1e+13, valid_loss=8.95e+11]              \n",
            "Epoch 304: 100%|██████████| 1/1 [00:00<00:00, 73.94it/s, v_num=0, train_loss_step=7.62e+12, train_loss_epoch=1e+13, valid_loss=8.95e+11]\n",
            "Epoch 305:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=7.62e+12, train_loss_epoch=7.62e+12, valid_loss=8.95e+11]        \n",
            "Epoch 313:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.18e+13, train_loss_epoch=1.18e+13, valid_loss=8.95e+11]        \n",
            "Epoch 320:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.16e+13, train_loss_epoch=1.16e+13, valid_loss=8.95e+11]        \n",
            "Epoch 321:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2e+12, train_loss_epoch=2e+12, valid_loss=8.95e+11]              \n",
            "Epoch 328:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.35e+13, train_loss_epoch=1.35e+13, valid_loss=8.95e+11]        \n",
            "Epoch 329:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.46e+11, train_loss_epoch=3.46e+11, valid_loss=8.95e+11]        \n",
            "Epoch 336:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5.02e+12, train_loss_epoch=5.02e+12, valid_loss=8.95e+11]        \n",
            "Epoch 337:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=9.46e+12, train_loss_epoch=9.46e+12, valid_loss=8.95e+11]        \n",
            "Epoch 345:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.13e+13, train_loss_epoch=1.13e+13, valid_loss=8.95e+11]         \n",
            "Epoch 352:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=6.77e+12, train_loss_epoch=6.77e+12, valid_loss=8.95e+11]        \n",
            "Epoch 353:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=6.3e+12, train_loss_epoch=6.3e+12, valid_loss=8.95e+11]          \n",
            "Epoch 360:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5.47e+12, train_loss_epoch=5.47e+12, valid_loss=8.95e+11]        \n",
            "Epoch 361:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=7.7e+12, train_loss_epoch=7.7e+12, valid_loss=8.95e+11]          \n",
            "Epoch 368:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5.11e+12, train_loss_epoch=5.11e+12, valid_loss=8.95e+11]        \n",
            "Epoch 369:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=6.09e+12, train_loss_epoch=6.09e+12, valid_loss=8.95e+11]        \n",
            "Epoch 376:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.26e+13, train_loss_epoch=1.26e+13, valid_loss=8.95e+11]        \n",
            "Epoch 376: 100%|██████████| 1/1 [00:00<00:00, 73.91it/s, v_num=0, train_loss_step=1.17e+12, train_loss_epoch=1.26e+13, valid_loss=8.95e+11]\n",
            "Epoch 377:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.17e+12, train_loss_epoch=1.17e+12, valid_loss=8.95e+11]        \n",
            "Epoch 385:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.39e+13, train_loss_epoch=1.39e+13, valid_loss=8.95e+11]        \n",
            "Epoch 392:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.95e+12, train_loss_epoch=2.95e+12, valid_loss=8.95e+11]        \n",
            "Epoch 393:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=8.3e+12, train_loss_epoch=8.3e+12, valid_loss=8.95e+11]          \n",
            "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 67.46it/s, v_num=0, train_loss_step=1.23e+13, train_loss_epoch=6.8e+12, valid_loss=8.95e+11] \n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 499.80it/s]\u001b[A\n",
            "Epoch 400:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.23e+13, train_loss_epoch=1.23e+13, valid_loss=2.11e+13]        \n",
            "Epoch 407:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=6.46e+10, train_loss_epoch=6.46e+10, valid_loss=2.11e+13]        \n",
            "Epoch 408:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.63e+13, train_loss_epoch=1.63e+13, valid_loss=2.11e+13]        \n",
            "Epoch 416:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.05e+12, train_loss_epoch=4.05e+12, valid_loss=2.11e+13]         \n",
            "Epoch 424:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=8.96e+12, train_loss_epoch=8.96e+12, valid_loss=2.11e+13]        \n",
            "Epoch 432:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.45e+12, train_loss_epoch=2.45e+12, valid_loss=2.11e+13]        \n",
            "Epoch 439:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.75e+10, train_loss_epoch=4.75e+10, valid_loss=2.11e+13]        \n",
            "Epoch 440:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=9.79e+11, train_loss_epoch=9.79e+11, valid_loss=2.11e+13]        \n",
            "Epoch 448:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.4e+12, train_loss_epoch=1.4e+12, valid_loss=2.11e+13]          \n",
            "Epoch 455:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.1e+12, train_loss_epoch=2.1e+12, valid_loss=2.11e+13]          \n",
            "Epoch 456:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.48e+12, train_loss_epoch=1.48e+12, valid_loss=2.11e+13]        \n",
            "Epoch 463: 100%|██████████| 1/1 [00:00<00:00, 71.36it/s, v_num=0, train_loss_step=1.14e+12, train_loss_epoch=1.14e+12, valid_loss=2.11e+13]\n",
            "Epoch 464:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.14e+12, train_loss_epoch=1.14e+12, valid_loss=2.11e+13]        \n",
            "Epoch 472:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.05e+12, train_loss_epoch=1.05e+12, valid_loss=2.11e+13]        \n",
            "Epoch 480:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.16e+12, train_loss_epoch=1.16e+12, valid_loss=2.11e+13]        \n",
            "Epoch 487:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5.3e+11, train_loss_epoch=5.3e+11, valid_loss=2.11e+13]          \n",
            "Epoch 488:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.44e+11, train_loss_epoch=3.44e+11, valid_loss=2.11e+13]        \n",
            "Epoch 496:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.62e+11, train_loss_epoch=1.62e+11, valid_loss=2.11e+13]        \n",
            "Epoch 499: 100%|██████████| 1/1 [00:00<00:00, 76.84it/s, v_num=0, train_loss_step=2.77e+11, train_loss_epoch=3.42e+11, valid_loss=2.11e+13]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 500.51it/s]\u001b[A\n",
            "Epoch 503:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.1e+11, train_loss_epoch=2.1e+11, valid_loss=1.15e+11]          \n",
            "Epoch 504:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.28e+11, train_loss_epoch=2.28e+11, valid_loss=1.15e+11]        \n",
            "Epoch 511:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.4e+10, train_loss_epoch=4.4e+10, valid_loss=1.15e+11]          \n",
            "Epoch 512:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.09e+11, train_loss_epoch=1.09e+11, valid_loss=1.15e+11]        \n",
            "Epoch 519:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.65e+11, train_loss_epoch=1.65e+11, valid_loss=1.15e+11]        \n",
            "Epoch 520:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.35e+11, train_loss_epoch=1.35e+11, valid_loss=1.15e+11]        \n",
            "Epoch 527:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.82e+11, train_loss_epoch=1.82e+11, valid_loss=1.15e+11]        \n",
            "Epoch 528:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.68e+11, train_loss_epoch=1.68e+11, valid_loss=1.15e+11]        \n",
            "Epoch 535:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.03e+11, train_loss_epoch=1.03e+11, valid_loss=1.15e+11]        \n",
            "Epoch 536:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.03e+11, train_loss_epoch=1.03e+11, valid_loss=1.15e+11]\n",
            "Epoch 544:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5.08e+10, train_loss_epoch=5.08e+10, valid_loss=1.15e+11]        \n",
            "Epoch 552:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.33e+11, train_loss_epoch=1.33e+11, valid_loss=1.15e+11]        \n",
            "Epoch 559:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=6.77e+10, train_loss_epoch=6.77e+10, valid_loss=1.15e+11]         \n",
            "Epoch 560:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=7.7e+10, train_loss_epoch=7.7e+10, valid_loss=1.15e+11]          \n",
            "Epoch 567:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.14e+11, train_loss_epoch=1.14e+11, valid_loss=1.15e+11]         \n",
            "Epoch 568:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.64e+11, train_loss_epoch=1.64e+11, valid_loss=1.15e+11]        \n",
            "Epoch 575: 100%|██████████| 1/1 [00:00<00:00, 68.61it/s, v_num=0, train_loss_step=1.72e+11, train_loss_epoch=9.78e+10, valid_loss=1.15e+11]\n",
            "Epoch 576:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.72e+11, train_loss_epoch=1.72e+11, valid_loss=1.15e+11]        \n",
            "Epoch 584:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=8.07e+9, train_loss_epoch=8.07e+9, valid_loss=1.15e+11]          \n",
            "Epoch 592:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=6.79e+10, train_loss_epoch=6.79e+10, valid_loss=1.15e+11]         \n",
            "Epoch 593:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5.62e+10, train_loss_epoch=5.62e+10, valid_loss=1.15e+11]        \n",
            "Epoch 599: 100%|██████████| 1/1 [00:00<00:00, 83.28it/s, v_num=0, train_loss_step=9.61e+10, train_loss_epoch=6.35e+10, valid_loss=1.15e+11]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 399.50it/s]\u001b[A\n",
            "Epoch 600:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=9.61e+10, train_loss_epoch=9.61e+10, valid_loss=6.16e+10]        \n",
            "Epoch 608:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.09e+10, train_loss_epoch=1.09e+10, valid_loss=6.16e+10]         \n",
            "Epoch 608: 100%|██████████| 1/1 [00:00<00:00, 76.78it/s, v_num=0, train_loss_step=5.3e+10, train_loss_epoch=5.3e+10, valid_loss=6.16e+10]  \n",
            "Epoch 609:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5.3e+10, train_loss_epoch=5.3e+10, valid_loss=6.16e+10]        \n",
            "Epoch 617:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=6.59e+10, train_loss_epoch=6.59e+10, valid_loss=6.16e+10]        \n",
            "Epoch 617: 100%|██████████| 1/1 [00:00<00:00, 73.98it/s, v_num=0, train_loss_step=1.02e+11, train_loss_epoch=1.02e+11, valid_loss=6.16e+10]\n",
            "Epoch 618:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.02e+11, train_loss_epoch=1.02e+11, valid_loss=6.16e+10]        \n",
            "Epoch 626:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.32e+10, train_loss_epoch=1.32e+10, valid_loss=6.16e+10]        \n",
            "Epoch 634:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.03e+11, train_loss_epoch=1.03e+11, valid_loss=6.16e+10]        \n",
            "Epoch 641:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=8.98e+10, train_loss_epoch=8.98e+10, valid_loss=6.16e+10]        \n",
            "Epoch 642:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.38e+11, train_loss_epoch=1.38e+11, valid_loss=6.16e+10]        \n",
            "Epoch 650:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.25e+11, train_loss_epoch=1.25e+11, valid_loss=6.16e+10]        \n",
            "Epoch 658:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.74e+11, train_loss_epoch=1.74e+11, valid_loss=6.16e+10]        \n",
            "Epoch 665:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.81e+10, train_loss_epoch=4.81e+10, valid_loss=6.16e+10]        \n",
            "Epoch 666:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=8.04e+10, train_loss_epoch=8.04e+10, valid_loss=6.16e+10]        \n",
            "Epoch 673:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=7.14e+10, train_loss_epoch=7.14e+10, valid_loss=6.16e+10]        \n",
            "Epoch 674:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.76e+10, train_loss_epoch=1.76e+10, valid_loss=6.16e+10]        \n",
            "Epoch 682:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=8.69e+10, train_loss_epoch=8.69e+10, valid_loss=6.16e+10]        \n",
            "Epoch 690:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=8.99e+10, train_loss_epoch=8.99e+10, valid_loss=6.16e+10]        \n",
            "Epoch 698:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=8.64e+9, train_loss_epoch=8.64e+9, valid_loss=6.16e+10]          \n",
            "Epoch 699: 100%|██████████| 1/1 [00:00<00:00, 71.31it/s, v_num=0, train_loss_step=1.97e+10, train_loss_epoch=4.88e+10, valid_loss=6.16e+10]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 90.16it/s]\u001b[A\n",
            "Epoch 704:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.48e+11, train_loss_epoch=1.48e+11, valid_loss=8.17e+10]        \n",
            "Epoch 705:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.48e+11, train_loss_epoch=1.48e+11, valid_loss=8.17e+10]\n",
            "Epoch 713:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.78e+11, train_loss_epoch=1.78e+11, valid_loss=8.17e+10]        \n",
            "Epoch 720:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=7.52e+10, train_loss_epoch=7.52e+10, valid_loss=8.17e+10]        \n",
            "Epoch 721:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.53e+10, train_loss_epoch=1.53e+10, valid_loss=8.17e+10]        \n",
            "Epoch 726: 100%|██████████| 1/1 [00:00<00:00, 76.84it/s, v_num=0, train_loss_step=7.61e+10, train_loss_epoch=7.61e+10, valid_loss=8.17e+10]\n",
            "Epoch 727:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.47e+10, train_loss_epoch=3.47e+10, valid_loss=8.17e+10]        \n",
            "Epoch 734:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.61e+10, train_loss_epoch=1.61e+10, valid_loss=8.17e+10]        \n",
            "Epoch 735:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.54e+11, train_loss_epoch=1.54e+11, valid_loss=8.17e+10]        \n",
            "Epoch 743:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.62e+10, train_loss_epoch=4.62e+10, valid_loss=8.17e+10]        \n",
            "Epoch 751:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=6.04e+10, train_loss_epoch=6.04e+10, valid_loss=8.17e+10]        \n",
            "Epoch 759:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.34e+10, train_loss_epoch=3.34e+10, valid_loss=8.17e+10]        \n",
            "Epoch 767:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.92e+10, train_loss_epoch=4.92e+10, valid_loss=8.17e+10]        \n",
            "Epoch 773:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5.95e+10, train_loss_epoch=5.95e+10, valid_loss=8.17e+10]        \n",
            "Epoch 781:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=6.44e+10, train_loss_epoch=6.44e+10, valid_loss=8.17e+10]        \n",
            "Epoch 789:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.94e+10, train_loss_epoch=4.94e+10, valid_loss=8.17e+10]        \n",
            "Epoch 796:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.99e+10, train_loss_epoch=1.99e+10, valid_loss=8.17e+10]        \n",
            "Epoch 799: 100%|██████████| 1/1 [00:00<00:00, 83.22it/s, v_num=0, train_loss_step=1.94e+10, train_loss_epoch=2.13e+10, valid_loss=8.17e+10]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\u001b[A\n",
            "Epoch 803:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.27e+10, train_loss_epoch=4.27e+10, valid_loss=5.87e+10]        \n",
            "Epoch 811:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.45e+10, train_loss_epoch=2.45e+10, valid_loss=5.87e+10]        \n",
            "Epoch 819:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.06e+10, train_loss_epoch=3.06e+10, valid_loss=5.87e+10]        \n",
            "Epoch 826:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.84e+10, train_loss_epoch=4.84e+10, valid_loss=5.87e+10]        \n",
            "Epoch 826: 100%|██████████| 1/1 [00:00<00:00, 64.43it/s, v_num=0, train_loss_step=2.54e+10, train_loss_epoch=4.84e+10, valid_loss=5.87e+10]\n",
            "Epoch 827:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.54e+10, train_loss_epoch=2.54e+10, valid_loss=5.87e+10]        \n",
            "Epoch 834:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5e+10, train_loss_epoch=5e+10, valid_loss=5.87e+10]              \n",
            "Epoch 835:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.59e+10, train_loss_epoch=2.59e+10, valid_loss=5.87e+10]        \n",
            "Epoch 843:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.15e+10, train_loss_epoch=3.15e+10, valid_loss=5.87e+10]         \n",
            "Epoch 850:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5.6e+10, train_loss_epoch=5.6e+10, valid_loss=5.87e+10]          \n",
            "Epoch 851:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.41e+10, train_loss_epoch=2.41e+10, valid_loss=5.87e+10]        \n",
            "Epoch 858:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.54e+10, train_loss_epoch=3.54e+10, valid_loss=5.87e+10]        \n",
            "Epoch 859:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5.77e+10, train_loss_epoch=5.77e+10, valid_loss=5.87e+10]        \n",
            "Epoch 866:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5.72e+10, train_loss_epoch=5.72e+10, valid_loss=5.87e+10]        \n",
            "Epoch 867:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.3e+10, train_loss_epoch=1.3e+10, valid_loss=5.87e+10]          \n",
            "Epoch 875:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5.56e+10, train_loss_epoch=5.56e+10, valid_loss=5.87e+10]         \n",
            "Epoch 882:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=9.45e+10, train_loss_epoch=9.45e+10, valid_loss=5.87e+10]        \n",
            "Epoch 890:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.42e+11, train_loss_epoch=1.42e+11, valid_loss=5.87e+10]         \n",
            "Epoch 897: 100%|██████████| 1/1 [00:00<00:00, 79.87it/s, v_num=0, train_loss_step=2.2e+10, train_loss_epoch=2.2e+10, valid_loss=5.87e+10]  \n",
            "Epoch 898:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.2e+10, train_loss_epoch=2.2e+10, valid_loss=5.87e+10]        \n",
            "Epoch 899: 100%|██████████| 1/1 [00:00<00:00, 79.93it/s, v_num=0, train_loss_step=9.65e+10, train_loss_epoch=8.15e+10, valid_loss=5.87e+10]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 499.38it/s]\u001b[A\n",
            "Epoch 905:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.95e+10, train_loss_epoch=2.95e+10, valid_loss=2.43e+10]        \n",
            "Epoch 913:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.13e+11, train_loss_epoch=1.13e+11, valid_loss=2.43e+10]        \n",
            "Epoch 921:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.03e+11, train_loss_epoch=1.03e+11, valid_loss=2.43e+10]        \n",
            "Epoch 929:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.02e+11, train_loss_epoch=1.02e+11, valid_loss=2.43e+10]        \n",
            "Epoch 936:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.08e+11, train_loss_epoch=1.08e+11, valid_loss=2.43e+10]        \n",
            "Epoch 936: 100%|██████████| 1/1 [00:00<00:00, 73.91it/s, v_num=0, train_loss_step=2.64e+10, train_loss_epoch=2.64e+10, valid_loss=2.43e+10]\n",
            "Epoch 937:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.64e+10, train_loss_epoch=2.64e+10, valid_loss=2.43e+10]        \n",
            "Epoch 945:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.16e+11, train_loss_epoch=1.16e+11, valid_loss=2.43e+10]         \n",
            "Epoch 952:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=8.53e+10, train_loss_epoch=8.53e+10, valid_loss=2.43e+10]        \n",
            "Epoch 952: 100%|██████████| 1/1 [00:00<00:00, 68.74it/s, v_num=0, train_loss_step=8.53e+10, train_loss_epoch=8.53e+10, valid_loss=2.43e+10]\n",
            "Epoch 953:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.01e+10, train_loss_epoch=3.01e+10, valid_loss=2.43e+10]        \n",
            "Epoch 960:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.31e+10, train_loss_epoch=4.31e+10, valid_loss=2.43e+10]        \n",
            "Epoch 967:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.83e+10, train_loss_epoch=3.83e+10, valid_loss=2.43e+10]        \n",
            "Epoch 973:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.37e+10, train_loss_epoch=1.37e+10, valid_loss=2.43e+10]        \n",
            "Epoch 974:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.8e+10, train_loss_epoch=4.8e+10, valid_loss=2.43e+10]          \n",
            "Epoch 980:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.9e+10, train_loss_epoch=3.9e+10, valid_loss=2.43e+10]          \n",
            "Epoch 986: 100%|██████████| 1/1 [00:00<00:00, 71.33it/s, v_num=0, train_loss_step=2.78e+10, train_loss_epoch=2.78e+10, valid_loss=2.43e+10]\n",
            "Epoch 987:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.78e+10, train_loss_epoch=2.78e+10, valid_loss=2.43e+10]        \n",
            "Epoch 994:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=8.54e+9, train_loss_epoch=8.54e+9, valid_loss=2.43e+10]          \n",
            "Epoch 995:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.31e+10, train_loss_epoch=2.31e+10, valid_loss=2.43e+10]        \n",
            "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 64.47it/s, v_num=0, train_loss_step=3.49e+10, train_loss_epoch=9.98e+9, valid_loss=2.43e+10] \n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 398.55it/s]\u001b[A\n",
            "Epoch 1001:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.81e+10, train_loss_epoch=3.81e+10, valid_loss=3.33e+10]        \n",
            "Epoch 1002:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5.63e+9, train_loss_epoch=5.63e+9, valid_loss=3.33e+10]          \n",
            "Epoch 1009:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5.73e+10, train_loss_epoch=5.73e+10, valid_loss=3.33e+10]        \n",
            "Epoch 1016:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.57e+10, train_loss_epoch=3.57e+10, valid_loss=3.33e+10]        \n",
            "Epoch 1017:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.64e+10, train_loss_epoch=4.64e+10, valid_loss=3.33e+10]        \n",
            "Epoch 1025:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.85e+10, train_loss_epoch=2.85e+10, valid_loss=3.33e+10]         \n",
            "Epoch 1032:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.67e+10, train_loss_epoch=4.67e+10, valid_loss=3.33e+10]        \n",
            "Epoch 1033:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.23e+10, train_loss_epoch=1.23e+10, valid_loss=3.33e+10]        \n",
            "Epoch 1040:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.04e+10, train_loss_epoch=1.04e+10, valid_loss=3.33e+10]        \n",
            "Epoch 1041:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=8.91e+9, train_loss_epoch=8.91e+9, valid_loss=3.33e+10]          \n",
            "Epoch 1048:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=7.67e+9, train_loss_epoch=7.67e+9, valid_loss=3.33e+10]          \n",
            "Epoch 1049:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.95e+10, train_loss_epoch=1.95e+10, valid_loss=3.33e+10]        \n",
            "Epoch 1056:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=6.4e+9, train_loss_epoch=6.4e+9, valid_loss=3.33e+10]            \n",
            "Epoch 1057:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.22e+10, train_loss_epoch=1.22e+10, valid_loss=3.33e+10]        \n",
            "Epoch 1064: 100%|██████████| 1/1 [00:00<00:00, 76.85it/s, v_num=0, train_loss_step=3.11e+10, train_loss_epoch=3.56e+10, valid_loss=3.33e+10]\n",
            "Epoch 1065:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.11e+10, train_loss_epoch=3.11e+10, valid_loss=3.33e+10]        \n",
            "Epoch 1073:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.12e+10, train_loss_epoch=3.12e+10, valid_loss=3.33e+10]        \n",
            "Epoch 1081:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=8.65e+9, train_loss_epoch=8.65e+9, valid_loss=3.33e+10]          \n",
            "Epoch 1089:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.28e+10, train_loss_epoch=1.28e+10, valid_loss=3.33e+10]        \n",
            "Epoch 1096:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.39e+10, train_loss_epoch=4.39e+10, valid_loss=3.33e+10]        \n",
            "Epoch 1097:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5.32e+10, train_loss_epoch=5.32e+10, valid_loss=3.33e+10]        \n",
            "Epoch 1099: 100%|██████████| 1/1 [00:00<00:00, 62.42it/s, v_num=0, train_loss_step=6.32e+10, train_loss_epoch=4.51e+10, valid_loss=3.33e+10]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\u001b[A\n",
            "Epoch 1104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.02e+10, train_loss_epoch=2.02e+10, valid_loss=5.26e+10]        \n",
            "Epoch 1105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.24e+10, train_loss_epoch=3.24e+10, valid_loss=5.26e+10]        \n",
            "Epoch 1112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5.03e+10, train_loss_epoch=5.03e+10, valid_loss=5.26e+10]        \n",
            "Epoch 1113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.07e+10, train_loss_epoch=3.07e+10, valid_loss=5.26e+10]        \n",
            "Epoch 1121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=8.13e+10, train_loss_epoch=8.13e+10, valid_loss=5.26e+10]        \n",
            "Epoch 1128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.51e+10, train_loss_epoch=3.51e+10, valid_loss=5.26e+10]        \n",
            "Epoch 1136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.87e+10, train_loss_epoch=2.87e+10, valid_loss=5.26e+10]        \n",
            "Epoch 1143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=6.16e+10, train_loss_epoch=6.16e+10, valid_loss=5.26e+10]        \n",
            "Epoch 1144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.2e+10, train_loss_epoch=4.2e+10, valid_loss=5.26e+10]          \n",
            "Epoch 1151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5.03e+10, train_loss_epoch=5.03e+10, valid_loss=5.26e+10]         \n",
            "Epoch 1152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=6.72e+10, train_loss_epoch=6.72e+10, valid_loss=5.26e+10]        \n",
            "Epoch 1160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.72e+10, train_loss_epoch=4.72e+10, valid_loss=5.26e+10]        \n",
            "Epoch 1167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.45e+10, train_loss_epoch=3.45e+10, valid_loss=5.26e+10]        \n",
            "Epoch 1168:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.27e+10, train_loss_epoch=1.27e+10, valid_loss=5.26e+10]        \n",
            "Epoch 1175:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.41e+10, train_loss_epoch=1.41e+10, valid_loss=5.26e+10]        \n",
            "Epoch 1183:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=8.39e+9, train_loss_epoch=8.39e+9, valid_loss=5.26e+10]          \n",
            "Epoch 1184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=9.23e+9, train_loss_epoch=9.23e+9, valid_loss=5.26e+10]        \n",
            "Epoch 1191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=6.05e+9, train_loss_epoch=6.05e+9, valid_loss=5.26e+10]          \n",
            "Epoch 1192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.09e+10, train_loss_epoch=1.09e+10, valid_loss=5.26e+10]        \n",
            "Epoch 1199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=6.03e+9, train_loss_epoch=6.03e+9, valid_loss=5.26e+10]          \n",
            "Epoch 1199: 100%|██████████| 1/1 [00:00<00:00, 64.44it/s, v_num=0, train_loss_step=3.21e+10, train_loss_epoch=6.03e+9, valid_loss=5.26e+10]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(_train_tune pid=32564)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 332.56it/s]\u001b[A\n",
            "Epoch 1199: 100%|██████████| 1/1 [00:00<00:00, 42.50it/s, v_num=0, train_loss_step=3.21e+10, train_loss_epoch=3.21e+10, valid_loss=2.81e+10]\n",
            "Epoch 1200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.21e+10, train_loss_epoch=3.21e+10, valid_loss=2.81e+10]        \n",
            "Epoch 1208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.37e+10, train_loss_epoch=2.37e+10, valid_loss=2.81e+10]        \n",
            "Epoch 1215: 100%|██████████| 1/1 [00:00<00:00, 67.98it/s, v_num=0, train_loss_step=1.93e+10, train_loss_epoch=1.93e+10, valid_loss=2.81e+10]\n",
            "Epoch 1216:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.93e+10, train_loss_epoch=1.93e+10, valid_loss=2.81e+10]        \n",
            "Epoch 1223:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.83e+10, train_loss_epoch=1.83e+10, valid_loss=2.81e+10]        \n",
            "Epoch 1223: 100%|██████████| 1/1 [00:00<00:00, 64.43it/s, v_num=0, train_loss_step=2.4e+10, train_loss_epoch=2.4e+10, valid_loss=2.81e+10]  \n",
            "Epoch 1224:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.4e+10, train_loss_epoch=2.4e+10, valid_loss=2.81e+10]        \n",
            "Epoch 1232:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.27e+10, train_loss_epoch=3.27e+10, valid_loss=2.81e+10]         \n",
            "Epoch 1240:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.77e+10, train_loss_epoch=1.77e+10, valid_loss=2.81e+10]        \n",
            "Epoch 1241:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.81e+10, train_loss_epoch=1.81e+10, valid_loss=2.81e+10]        \n",
            "Epoch 1248:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.59e+10, train_loss_epoch=2.59e+10, valid_loss=2.81e+10]        \n",
            "Epoch 1249:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.56e+10, train_loss_epoch=2.56e+10, valid_loss=2.81e+10]        \n",
            "Epoch 1257:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.68e+10, train_loss_epoch=1.68e+10, valid_loss=2.81e+10]        \n",
            "Epoch 1264:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.9e+10, train_loss_epoch=2.9e+10, valid_loss=2.81e+10]          \n",
            "Epoch 1272:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.88e+10, train_loss_epoch=2.88e+10, valid_loss=2.81e+10]        \n",
            "Epoch 1280:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.5e+10, train_loss_epoch=2.5e+10, valid_loss=2.81e+10]           \n",
            "Epoch 1288:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.53e+10, train_loss_epoch=2.53e+10, valid_loss=2.81e+10]        \n",
            "Epoch 1295: 100%|██████████| 1/1 [00:00<00:00, 73.38it/s, v_num=0, train_loss_step=1.99e+10, train_loss_epoch=1.99e+10, valid_loss=2.81e+10]\n",
            "Epoch 1296:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.99e+10, train_loss_epoch=1.99e+10, valid_loss=2.81e+10]        \n",
            "Epoch 1299: 100%|██████████| 1/1 [00:00<00:00, 71.17it/s, v_num=0, train_loss_step=2.81e+10, train_loss_epoch=2.36e+10, valid_loss=2.81e+10]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 499.20it/s]\u001b[A\n",
            "Epoch 1303:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=9.93e+9, train_loss_epoch=9.93e+9, valid_loss=3.1e+10]           \n",
            "Epoch 1303:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.51e+10, train_loss_epoch=2.51e+10, valid_loss=3.1e+10]        \n",
            "Epoch 1304:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.51e+10, train_loss_epoch=2.51e+10, valid_loss=3.1e+10]\n",
            "Epoch 1311:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.8e+10, train_loss_epoch=1.8e+10, valid_loss=3.1e+10]          \n",
            "Epoch 1312:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.25e+10, train_loss_epoch=3.25e+10, valid_loss=3.1e+10]        \n",
            "Epoch 1320:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.93e+10, train_loss_epoch=3.93e+10, valid_loss=3.1e+10]        \n",
            "Epoch 1328:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.22e+10, train_loss_epoch=1.22e+10, valid_loss=3.1e+10]         \n",
            "Epoch 1336:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.73e+10, train_loss_epoch=4.73e+10, valid_loss=3.1e+10]        \n",
            "Epoch 1344:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5.26e+10, train_loss_epoch=5.26e+10, valid_loss=3.1e+10]         \n",
            "Epoch 1345:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=6.82e+10, train_loss_epoch=6.82e+10, valid_loss=3.1e+10]        \n",
            "Epoch 1352:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.38e+10, train_loss_epoch=4.38e+10, valid_loss=3.1e+10]        \n",
            "Epoch 1353:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.49e+10, train_loss_epoch=2.49e+10, valid_loss=3.1e+10]        \n",
            "Epoch 1360:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=6.32e+10, train_loss_epoch=6.32e+10, valid_loss=3.1e+10]        \n",
            "Epoch 1361:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=3.49e+10, train_loss_epoch=3.49e+10, valid_loss=3.1e+10]        \n",
            "Epoch 1369:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=6.4e+10, train_loss_epoch=6.4e+10, valid_loss=3.1e+10]          \n",
            "Epoch 1377:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.55e+10, train_loss_epoch=2.55e+10, valid_loss=3.1e+10]        \n",
            "Epoch 1384:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=4.98e+10, train_loss_epoch=4.98e+10, valid_loss=3.1e+10]        \n",
            "Epoch 1385:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.28e+10, train_loss_epoch=2.28e+10, valid_loss=3.1e+10]        \n",
            "Epoch 1392:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=5.4e+10, train_loss_epoch=5.4e+10, valid_loss=3.1e+10]          \n",
            "Epoch 1392: 100%|██████████| 1/1 [00:00<00:00, 68.92it/s, v_num=0, train_loss_step=7e+10, train_loss_epoch=7e+10, valid_loss=3.1e+10]    \n",
            "Epoch 1393:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=7e+10, train_loss_epoch=7e+10, valid_loss=3.1e+10]        \n",
            "Epoch 1399: 100%|██████████| 1/1 [00:00<00:00, 79.94it/s, v_num=0, train_loss_step=5.95e+10, train_loss_epoch=7.96e+10, valid_loss=3.1e+10]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 399.19it/s]\u001b[A\n",
            "Epoch 1399: 100%|██████████| 1/1 [00:00<00:00, 47.57it/s, v_num=0, train_loss_step=5.95e+10, train_loss_epoch=5.95e+10, valid_loss=3.37e+10]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_train_tune pid=32564)\u001b[0m `Trainer.fit` stopped: `max_steps=1400.0` reached.\n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m c:\\Users\\woodw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\integration\\pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m Seed set to 15\n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m GPU available: False, used: False\n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m \n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m   | Name         | Type          | Params | Mode  | FLOPs\n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m ---------------------------------------------------------------\n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m 0 | loss         | MAE           | 0      | train | 0    \n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m 1 | padder_train | ConstantPad1d | 0      | train | 0    \n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m 2 | scaler       | TemporalNorm  | 0      | train | 0    \n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m 3 | blocks       | ModuleList    | 2.4 M  | train | 0    \n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m ---------------------------------------------------------------\n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m 2.4 M     Trainable params\n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m 2.4 M     Total params\n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m 9.720     Total estimated model params size (MB)\n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m 34        Modules in train mode\n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m 0         Total Flops\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n",
            "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.070, train_loss_epoch=2.070]        \n",
            "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.120, train_loss_epoch=2.120]        \n",
            "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=2.210, train_loss_epoch=2.210]        \n",
            "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.680, train_loss_epoch=1.680]        \n",
            "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.430, train_loss_epoch=1.430]        \n",
            "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.430, train_loss_epoch=1.430]        \n",
            "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.250, train_loss_epoch=1.250]        \n",
            "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.090, train_loss_epoch=1.090]        \n",
            "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120]        \n",
            "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020]        \n",
            "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.862, train_loss_epoch=0.862]        \n",
            "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.865, train_loss_epoch=0.865]        \n",
            "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.827, train_loss_epoch=0.827]        \n",
            "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.768, train_loss_epoch=0.768]        \n",
            "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.723, train_loss_epoch=0.723]        \n",
            "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.733, train_loss_epoch=0.733]        \n",
            "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.688, train_loss_epoch=0.688]        \n",
            "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.637, train_loss_epoch=0.637]        \n",
            "Epoch 35: 100%|██████████| 1/1 [00:00<00:00, 20.08it/s, v_num=0, train_loss_step=0.668, train_loss_epoch=0.668]\n",
            "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.668, train_loss_epoch=0.668]        \n",
            "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.578, train_loss_epoch=0.578]        \n",
            "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.546, train_loss_epoch=0.546]        \n",
            "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.624, train_loss_epoch=0.624]        \n",
            "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.529, train_loss_epoch=0.529]        \n",
            "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.602, train_loss_epoch=0.602]        \n",
            "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.551, train_loss_epoch=0.551]        \n",
            "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.506, train_loss_epoch=0.506]        \n",
            "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.507, train_loss_epoch=0.507]        \n",
            "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.421, train_loss_epoch=0.421]        \n",
            "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.423, train_loss_epoch=0.423]        \n",
            "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.371, train_loss_epoch=0.371]        \n",
            "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.389, train_loss_epoch=0.389]        \n",
            "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.360, train_loss_epoch=0.360]        \n",
            "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.350, train_loss_epoch=0.350]        \n",
            "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.328, train_loss_epoch=0.328]        \n",
            "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.360, train_loss_epoch=0.360]        \n",
            "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.344, train_loss_epoch=0.344]        \n",
            "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.355, train_loss_epoch=0.355]        \n",
            "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.308, train_loss_epoch=0.308]        \n",
            "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.305, train_loss_epoch=0.305]        \n",
            "Epoch 75: 100%|██████████| 1/1 [00:00<00:00, 19.05it/s, v_num=0, train_loss_step=0.289, train_loss_epoch=0.289]\n",
            "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.289, train_loss_epoch=0.289]        \n",
            "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.295, train_loss_epoch=0.295]        \n",
            "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.293, train_loss_epoch=0.293]        \n",
            "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.267, train_loss_epoch=0.267]        \n",
            "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.282, train_loss_epoch=0.282]        \n",
            "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.268, train_loss_epoch=0.268]        \n",
            "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.240, train_loss_epoch=0.240]        \n",
            "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.214, train_loss_epoch=0.214]        \n",
            "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.223, train_loss_epoch=0.223]        \n",
            "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.206, train_loss_epoch=0.206]        \n",
            "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.234, train_loss_epoch=0.234]        \n",
            "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.208, train_loss_epoch=0.208]        \n",
            "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 19.71it/s, v_num=0, train_loss_step=0.219, train_loss_epoch=0.208]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m \n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 332.85it/s]\u001b[A\n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m \n",
            "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.214, train_loss_epoch=0.214, valid_loss=1.09e+4]        \n",
            "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.226, train_loss_epoch=0.226, valid_loss=1.09e+4]        \n",
            "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.211, train_loss_epoch=0.211, valid_loss=1.09e+4]        \n",
            "Epoch 106:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.219, train_loss_epoch=0.219, valid_loss=1.09e+4]        \n",
            "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.223, train_loss_epoch=0.223, valid_loss=1.09e+4]        \n",
            "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.223, train_loss_epoch=0.223, valid_loss=1.09e+4]        \n",
            "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.227, train_loss_epoch=0.227, valid_loss=1.09e+4]        \n",
            "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.209, train_loss_epoch=0.209, valid_loss=1.09e+4]        \n",
            "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.174, train_loss_epoch=0.174, valid_loss=1.09e+4]        \n",
            "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.194, train_loss_epoch=0.194, valid_loss=1.09e+4]        \n",
            "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.186, train_loss_epoch=0.186, valid_loss=1.09e+4]        \n",
            "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.172, train_loss_epoch=0.172, valid_loss=1.09e+4]        \n",
            "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.179, train_loss_epoch=0.179, valid_loss=1.09e+4]        \n",
            "Epoch 127: 100%|██████████| 1/1 [00:00<00:00, 19.54it/s, v_num=0, train_loss_step=0.205, train_loss_epoch=0.162, valid_loss=1.09e+4]\n",
            "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.205, train_loss_epoch=0.205, valid_loss=1.09e+4]        \n",
            "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.195, train_loss_epoch=0.195, valid_loss=1.09e+4]        \n",
            "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.161, train_loss_epoch=0.161, valid_loss=1.09e+4]        \n",
            "Epoch 134:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.154, train_loss_epoch=0.154, valid_loss=1.09e+4]        \n",
            "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.162, train_loss_epoch=0.162, valid_loss=1.09e+4]        \n",
            "Epoch 139:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.148, train_loss_epoch=0.148, valid_loss=1.09e+4]        \n",
            "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139, valid_loss=1.09e+4]        \n",
            "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.128, train_loss_epoch=0.128, valid_loss=1.09e+4]        \n",
            "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.174, train_loss_epoch=0.174, valid_loss=1.09e+4]        \n",
            "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.143, train_loss_epoch=0.143, valid_loss=1.09e+4]        \n",
            "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.149, train_loss_epoch=0.149, valid_loss=1.09e+4]        \n",
            "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.164, train_loss_epoch=0.164, valid_loss=1.09e+4]        \n",
            "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.134, train_loss_epoch=0.134, valid_loss=1.09e+4]        \n",
            "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.145, train_loss_epoch=0.145, valid_loss=1.09e+4]        \n",
            "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.145, train_loss_epoch=0.145, valid_loss=1.09e+4]        \n",
            "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.165, train_loss_epoch=0.165, valid_loss=1.09e+4]        \n",
            "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.161, train_loss_epoch=0.161, valid_loss=1.09e+4]        \n",
            "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.170, train_loss_epoch=0.170, valid_loss=1.09e+4]        \n",
            "Epoch 163:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.180, train_loss_epoch=0.180, valid_loss=1.09e+4]        \n",
            "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.168, train_loss_epoch=0.168, valid_loss=1.09e+4]        \n",
            "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.213, train_loss_epoch=0.213, valid_loss=1.09e+4]        \n",
            "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.133, valid_loss=1.09e+4]        \n",
            "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.124, train_loss_epoch=0.124, valid_loss=1.09e+4]        \n",
            "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.106, train_loss_epoch=0.106, valid_loss=1.09e+4]        \n",
            "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.113, train_loss_epoch=0.113, valid_loss=1.09e+4]        \n",
            "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.093, train_loss_epoch=0.093, valid_loss=1.09e+4]          \n",
            "Epoch 180:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0755, train_loss_epoch=0.0755, valid_loss=1.09e+4]        \n",
            "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0714, train_loss_epoch=0.0714, valid_loss=1.09e+4]        \n",
            "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0739, train_loss_epoch=0.0739, valid_loss=1.09e+4]        \n",
            "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0698, train_loss_epoch=0.0698, valid_loss=1.09e+4]        \n",
            "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0728, train_loss_epoch=0.0728, valid_loss=1.09e+4]        \n",
            "Epoch 189:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0776, train_loss_epoch=0.0776, valid_loss=1.09e+4]        \n",
            "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.066, train_loss_epoch=0.066, valid_loss=1.09e+4]          \n",
            "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0698, train_loss_epoch=0.0698, valid_loss=1.09e+4]        \n",
            "Epoch 195:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0595, train_loss_epoch=0.0595, valid_loss=1.09e+4]        \n",
            "Epoch 198:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0635, train_loss_epoch=0.0635, valid_loss=1.09e+4]        \n",
            "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 20.26it/s, v_num=0, train_loss_step=0.0564, train_loss_epoch=0.0615, valid_loss=1.09e+4]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 399.15it/s]\u001b[A\n",
            "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0564, train_loss_epoch=0.0564, valid_loss=1.19e+4]        \n",
            "Epoch 202:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0646, train_loss_epoch=0.0646, valid_loss=1.19e+4]        \n",
            "Epoch 204:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0633, train_loss_epoch=0.0633, valid_loss=1.19e+4]        \n",
            "Epoch 206:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0683, train_loss_epoch=0.0683, valid_loss=1.19e+4]        \n",
            "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0563, train_loss_epoch=0.0563, valid_loss=1.19e+4]        \n",
            "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0597, train_loss_epoch=0.0597, valid_loss=1.19e+4]        \n",
            "Epoch 212:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0675, train_loss_epoch=0.0675, valid_loss=1.19e+4]        \n",
            "Epoch 215:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0605, train_loss_epoch=0.0605, valid_loss=1.19e+4]        \n",
            "Epoch 217:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0778, train_loss_epoch=0.0778, valid_loss=1.19e+4]        \n",
            "Epoch 219:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0611, train_loss_epoch=0.0611, valid_loss=1.19e+4]        \n",
            "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0661, train_loss_epoch=0.0661, valid_loss=1.19e+4]        \n",
            "Epoch 223:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0629, train_loss_epoch=0.0629, valid_loss=1.19e+4]        \n",
            "Epoch 224:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0655, train_loss_epoch=0.0655, valid_loss=1.19e+4]        \n",
            "Epoch 226:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0644, train_loss_epoch=0.0644, valid_loss=1.19e+4]        \n",
            "Epoch 228:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0581, train_loss_epoch=0.0581, valid_loss=1.19e+4]        \n",
            "Epoch 230:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0664, train_loss_epoch=0.0664, valid_loss=1.19e+4]        \n",
            "Epoch 232:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0614, train_loss_epoch=0.0614, valid_loss=1.19e+4]        \n",
            "Epoch 234:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0665, train_loss_epoch=0.0665, valid_loss=1.19e+4]        \n",
            "Epoch 235:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0703, train_loss_epoch=0.0703, valid_loss=1.19e+4]        \n",
            "Epoch 237:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.070, train_loss_epoch=0.070, valid_loss=1.19e+4]          \n",
            "Epoch 239:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0663, train_loss_epoch=0.0663, valid_loss=1.19e+4]        \n",
            "Epoch 241:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0695, train_loss_epoch=0.0695, valid_loss=1.19e+4]        \n",
            "Epoch 243:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0665, train_loss_epoch=0.0665, valid_loss=1.19e+4]        \n",
            "Epoch 245:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0745, train_loss_epoch=0.0745, valid_loss=1.19e+4]        \n",
            "Epoch 246:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0571, train_loss_epoch=0.0571, valid_loss=1.19e+4]        \n",
            "Epoch 248:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0553, train_loss_epoch=0.0553, valid_loss=1.19e+4]        \n",
            "Epoch 250:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0578, train_loss_epoch=0.0578, valid_loss=1.19e+4]        \n",
            "Epoch 252:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0533, train_loss_epoch=0.0533, valid_loss=1.19e+4]        \n",
            "Epoch 255:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0724, train_loss_epoch=0.0724, valid_loss=1.19e+4]        \n",
            "Epoch 257:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.051, train_loss_epoch=0.051, valid_loss=1.19e+4]          \n",
            "Epoch 259:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0572, train_loss_epoch=0.0572, valid_loss=1.19e+4]        \n",
            "Epoch 261:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0545, train_loss_epoch=0.0545, valid_loss=1.19e+4]        \n",
            "Epoch 263:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.057, train_loss_epoch=0.057, valid_loss=1.19e+4]          \n",
            "Epoch 266:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0593, train_loss_epoch=0.0593, valid_loss=1.19e+4]        \n",
            "Epoch 268:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0591, train_loss_epoch=0.0591, valid_loss=1.19e+4]        \n",
            "Epoch 270:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.053, train_loss_epoch=0.053, valid_loss=1.19e+4]          \n",
            "Epoch 272:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0528, train_loss_epoch=0.0528, valid_loss=1.19e+4]        \n",
            "Epoch 275:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0532, train_loss_epoch=0.0532, valid_loss=1.19e+4]        \n",
            "Epoch 277:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.057, train_loss_epoch=0.057, valid_loss=1.19e+4]          \n",
            "Epoch 279:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0588, train_loss_epoch=0.0588, valid_loss=1.19e+4]        \n",
            "Epoch 281:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0622, train_loss_epoch=0.0622, valid_loss=1.19e+4]        \n",
            "Epoch 284:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0563, train_loss_epoch=0.0563, valid_loss=1.19e+4]        \n",
            "Epoch 286:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0608, train_loss_epoch=0.0608, valid_loss=1.19e+4]        \n",
            "Epoch 288:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.051, train_loss_epoch=0.051, valid_loss=1.19e+4]          \n",
            "Epoch 290:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0532, train_loss_epoch=0.0532, valid_loss=1.19e+4]        \n",
            "Epoch 292:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0574, train_loss_epoch=0.0574, valid_loss=1.19e+4]        \n",
            "Epoch 294:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0577, train_loss_epoch=0.0577, valid_loss=1.19e+4]        \n",
            "Epoch 296: 100%|██████████| 1/1 [00:00<00:00, 19.93it/s, v_num=0, train_loss_step=0.0459, train_loss_epoch=0.0459, valid_loss=1.19e+4]\n",
            "Epoch 297:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0497, train_loss_epoch=0.0497, valid_loss=1.19e+4]        \n",
            "Epoch 299:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0495, train_loss_epoch=0.0495, valid_loss=1.19e+4]        \n",
            "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 19.98it/s, v_num=0, train_loss_step=0.0557, train_loss_epoch=0.0495, valid_loss=1.19e+4]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 498.73it/s]\u001b[A\n",
            "Epoch 301:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0636, train_loss_epoch=0.0636, valid_loss=1.2e+4]         \n",
            "Epoch 303:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0588, train_loss_epoch=0.0588, valid_loss=1.2e+4]        \n",
            "Epoch 305:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.060, train_loss_epoch=0.060, valid_loss=1.2e+4]          \n",
            "Epoch 307:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0659, train_loss_epoch=0.0659, valid_loss=1.2e+4]        \n",
            "Epoch 309:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0683, train_loss_epoch=0.0683, valid_loss=1.2e+4]        \n",
            "Epoch 311:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0667, train_loss_epoch=0.0667, valid_loss=1.2e+4]        \n",
            "Epoch 314:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0534, train_loss_epoch=0.0534, valid_loss=1.2e+4]        \n",
            "Epoch 316:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0531, train_loss_epoch=0.0531, valid_loss=1.2e+4]        \n",
            "Epoch 318:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.049, train_loss_epoch=0.049, valid_loss=1.2e+4]          \n",
            "Epoch 320:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0522, train_loss_epoch=0.0522, valid_loss=1.2e+4]        \n",
            "Epoch 322:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0461, train_loss_epoch=0.0461, valid_loss=1.2e+4]        \n",
            "Epoch 324:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0541, train_loss_epoch=0.0541, valid_loss=1.2e+4]        \n",
            "Epoch 325:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0525, train_loss_epoch=0.0525, valid_loss=1.2e+4]        \n",
            "Epoch 327:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0475, train_loss_epoch=0.0475, valid_loss=1.2e+4]        \n",
            "Epoch 329:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.050, train_loss_epoch=0.050, valid_loss=1.2e+4]          \n",
            "Epoch 331:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0459, train_loss_epoch=0.0459, valid_loss=1.2e+4]        \n",
            "Epoch 333:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0513, train_loss_epoch=0.0513, valid_loss=1.2e+4]        \n",
            "Epoch 335:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0441, train_loss_epoch=0.0441, valid_loss=1.2e+4]        \n",
            "Epoch 337:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0401, train_loss_epoch=0.0401, valid_loss=1.2e+4]        \n",
            "Epoch 339:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0327, train_loss_epoch=0.0327, valid_loss=1.2e+4]        \n",
            "Epoch 341:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0307, train_loss_epoch=0.0307, valid_loss=1.2e+4]        \n",
            "Epoch 342:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0337, train_loss_epoch=0.0337, valid_loss=1.2e+4]        \n",
            "Epoch 344:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0297, train_loss_epoch=0.0297, valid_loss=1.2e+4]        \n",
            "Epoch 346:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0331, train_loss_epoch=0.0331, valid_loss=1.2e+4]        \n",
            "Epoch 348:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0281, train_loss_epoch=0.0281, valid_loss=1.2e+4]        \n",
            "Epoch 350:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0296, train_loss_epoch=0.0296, valid_loss=1.2e+4]        \n",
            "Epoch 352:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0289, train_loss_epoch=0.0289, valid_loss=1.2e+4]        \n",
            "Epoch 354:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0253, train_loss_epoch=0.0253, valid_loss=1.2e+4]        \n",
            "Epoch 356:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0316, train_loss_epoch=0.0316, valid_loss=1.2e+4]        \n",
            "Epoch 358:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0286, train_loss_epoch=0.0286, valid_loss=1.2e+4]        \n",
            "Epoch 361:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0256, train_loss_epoch=0.0256, valid_loss=1.2e+4]        \n",
            "Epoch 363:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0254, train_loss_epoch=0.0254, valid_loss=1.2e+4]        \n",
            "Epoch 365:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0264, train_loss_epoch=0.0264, valid_loss=1.2e+4]        \n",
            "Epoch 367:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0278, train_loss_epoch=0.0278, valid_loss=1.2e+4]        \n",
            "Epoch 369:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0248, train_loss_epoch=0.0248, valid_loss=1.2e+4]        \n",
            "Epoch 371:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0315, train_loss_epoch=0.0315, valid_loss=1.2e+4]        \n",
            "Epoch 372:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0241, train_loss_epoch=0.0241, valid_loss=1.2e+4]        \n",
            "Epoch 374:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0273, train_loss_epoch=0.0273, valid_loss=1.2e+4]        \n",
            "Epoch 376:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0287, train_loss_epoch=0.0287, valid_loss=1.2e+4]        \n",
            "Epoch 378:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0273, train_loss_epoch=0.0273, valid_loss=1.2e+4]        \n",
            "Epoch 380:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0279, train_loss_epoch=0.0279, valid_loss=1.2e+4]        \n",
            "Epoch 382:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0298, train_loss_epoch=0.0298, valid_loss=1.2e+4]        \n",
            "Epoch 384:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0328, train_loss_epoch=0.0328, valid_loss=1.2e+4]        \n",
            "Epoch 384: 100%|██████████| 1/1 [00:00<00:00, 19.98it/s, v_num=0, train_loss_step=0.0333, train_loss_epoch=0.0333, valid_loss=1.2e+4]\n",
            "Epoch 385:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0333, train_loss_epoch=0.0333, valid_loss=1.2e+4]        \n",
            "Epoch 387:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0305, train_loss_epoch=0.0305, valid_loss=1.2e+4]        \n",
            "Epoch 389:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.036, train_loss_epoch=0.036, valid_loss=1.2e+4]          \n",
            "Epoch 391:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0333, train_loss_epoch=0.0333, valid_loss=1.2e+4]        \n",
            "Epoch 393:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0338, train_loss_epoch=0.0338, valid_loss=1.2e+4]        \n",
            "Epoch 394:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0368, train_loss_epoch=0.0368, valid_loss=1.2e+4]        \n",
            "Epoch 396:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0297, train_loss_epoch=0.0297, valid_loss=1.2e+4]        \n",
            "Epoch 398:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0292, train_loss_epoch=0.0292, valid_loss=1.2e+4]        \n",
            "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 21.14it/s, v_num=0, train_loss_step=0.0296, train_loss_epoch=0.032, valid_loss=1.2e+4] \n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 285.21it/s]\u001b[A\n",
            "Epoch 400:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0296, train_loss_epoch=0.0296, valid_loss=1.17e+4]        \n",
            "Epoch 402:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0297, train_loss_epoch=0.0297, valid_loss=1.17e+4]        \n",
            "Epoch 404:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0281, train_loss_epoch=0.0281, valid_loss=1.17e+4]        \n",
            "Epoch 406:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0284, train_loss_epoch=0.0284, valid_loss=1.17e+4]        \n",
            "Epoch 408:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0285, train_loss_epoch=0.0285, valid_loss=1.17e+4]        \n",
            "Epoch 410:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0271, train_loss_epoch=0.0271, valid_loss=1.17e+4]        \n",
            "Epoch 411:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0297, train_loss_epoch=0.0297, valid_loss=1.17e+4]        \n",
            "Epoch 413:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0285, train_loss_epoch=0.0285, valid_loss=1.17e+4]        \n",
            "Epoch 415:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0317, train_loss_epoch=0.0317, valid_loss=1.17e+4]        \n",
            "Epoch 417:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0294, train_loss_epoch=0.0294, valid_loss=1.17e+4]        \n",
            "Epoch 419:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0312, train_loss_epoch=0.0312, valid_loss=1.17e+4]        \n",
            "Epoch 421:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0309, train_loss_epoch=0.0309, valid_loss=1.17e+4]        \n",
            "Epoch 424:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0262, train_loss_epoch=0.0262, valid_loss=1.17e+4]        \n",
            "Epoch 426:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0292, train_loss_epoch=0.0292, valid_loss=1.17e+4]        \n",
            "Epoch 428:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0298, train_loss_epoch=0.0298, valid_loss=1.17e+4]        \n",
            "Epoch 430:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0279, train_loss_epoch=0.0279, valid_loss=1.17e+4]        \n",
            "Epoch 432:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0246, train_loss_epoch=0.0246, valid_loss=1.17e+4]        \n",
            "Epoch 434:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0301, train_loss_epoch=0.0301, valid_loss=1.17e+4]        \n",
            "Epoch 437:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0257, train_loss_epoch=0.0257, valid_loss=1.17e+4]        \n",
            "Epoch 439:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0252, train_loss_epoch=0.0252, valid_loss=1.17e+4]        \n",
            "Epoch 441:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0266, train_loss_epoch=0.0266, valid_loss=1.17e+4]        \n",
            "Epoch 443:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0276, train_loss_epoch=0.0276, valid_loss=1.17e+4]        \n",
            "Epoch 445:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0291, train_loss_epoch=0.0291, valid_loss=1.17e+4]        \n",
            "Epoch 447:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0236, train_loss_epoch=0.0236, valid_loss=1.17e+4]        \n",
            "Epoch 449:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0251, train_loss_epoch=0.0251, valid_loss=1.17e+4]        \n",
            "Epoch 451:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0278, train_loss_epoch=0.0278, valid_loss=1.17e+4]        \n",
            "Epoch 452:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0256, train_loss_epoch=0.0256, valid_loss=1.17e+4]        \n",
            "Epoch 454:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.026, train_loss_epoch=0.026, valid_loss=1.17e+4]          \n",
            "Epoch 456:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0244, train_loss_epoch=0.0244, valid_loss=1.17e+4]        \n",
            "Epoch 458:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0251, train_loss_epoch=0.0251, valid_loss=1.17e+4]        \n",
            "Epoch 458: 100%|██████████| 1/1 [00:00<00:00, 19.18it/s, v_num=0, train_loss_step=0.0249, train_loss_epoch=0.0251, valid_loss=1.17e+4]\n",
            "Epoch 459:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0249, train_loss_epoch=0.0249, valid_loss=1.17e+4]        \n",
            "Epoch 461:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0263, train_loss_epoch=0.0263, valid_loss=1.17e+4]        \n",
            "Epoch 463:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0257, train_loss_epoch=0.0257, valid_loss=1.17e+4]        \n",
            "Epoch 465:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0276, train_loss_epoch=0.0276, valid_loss=1.17e+4]        \n",
            "Epoch 467:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0275, train_loss_epoch=0.0275, valid_loss=1.17e+4]        \n",
            "Epoch 468:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0293, train_loss_epoch=0.0293, valid_loss=1.17e+4]        \n",
            "Epoch 470:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0233, train_loss_epoch=0.0233, valid_loss=1.17e+4]        \n",
            "Epoch 472:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0253, train_loss_epoch=0.0253, valid_loss=1.17e+4]        \n",
            "Epoch 474:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0252, train_loss_epoch=0.0252, valid_loss=1.17e+4]        \n",
            "Epoch 476:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0261, train_loss_epoch=0.0261, valid_loss=1.17e+4]        \n",
            "Epoch 477:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0284, train_loss_epoch=0.0284, valid_loss=1.17e+4]        \n",
            "Epoch 479:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.029, train_loss_epoch=0.029, valid_loss=1.17e+4]          \n",
            "Epoch 481:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0267, train_loss_epoch=0.0267, valid_loss=1.17e+4]        \n",
            "Epoch 483:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0314, train_loss_epoch=0.0314, valid_loss=1.17e+4]        \n",
            "Epoch 486:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0319, train_loss_epoch=0.0319, valid_loss=1.17e+4]        \n",
            "Epoch 488:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0241, train_loss_epoch=0.0241, valid_loss=1.17e+4]        \n",
            "Epoch 490:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0247, train_loss_epoch=0.0247, valid_loss=1.17e+4]        \n",
            "Epoch 492:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0241, train_loss_epoch=0.0241, valid_loss=1.17e+4]        \n",
            "Epoch 494:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0268, train_loss_epoch=0.0268, valid_loss=1.17e+4]        \n",
            "Epoch 495:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0224, train_loss_epoch=0.0224, valid_loss=1.17e+4]        \n",
            "Epoch 497:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0307, train_loss_epoch=0.0307, valid_loss=1.17e+4]        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-04 14:39:39,326\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/woodw/ray_results/_train_tune_2025-12-04_14-38-41' in 0.0071s.\n",
            "Seed set to 15\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\n",
            "  | Name         | Type          | Params | Mode  | FLOPs\n",
            "---------------------------------------------------------------\n",
            "0 | loss         | MAE           | 0      | eval  | 0    \n",
            "1 | padder_train | ConstantPad1d | 0      | train | 0    \n",
            "2 | scaler       | TemporalNorm  | 0      | train | 0    \n",
            "3 | blocks       | ModuleList    | 2.4 M  | train | 0    \n",
            "---------------------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.720     Total estimated model params size (MB)\n",
            "33        Modules in train mode\n",
            "1         Modules in eval mode\n",
            "0         Total Flops\n",
            "\u001b[36m(_train_tune pid=51864)\u001b[0m `Trainer.fit` stopped: `max_steps=500.0` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 499:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0298, train_loss_epoch=0.0298, valid_loss=1.17e+4]        \n",
            "Epoch 499: 100%|██████████| 1/1 [00:00<00:00, 19.19it/s, v_num=0, train_loss_step=0.0263, train_loss_epoch=0.0298, valid_loss=1.17e+4]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 249.93it/s]\u001b[A\n",
            "Epoch 499: 100%|██████████| 1/1 [00:00<00:00, 15.97it/s, v_num=0, train_loss_step=0.0263, train_loss_epoch=0.0263, valid_loss=1.18e+4]\n",
            "Epoch 499: 100%|██████████| 1/1 [00:00<00:00, 15.75it/s, v_num=4, train_loss_step=0.0246, train_loss_epoch=0.0246]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_steps=500.0` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 499: 100%|██████████| 1/1 [00:00<00:00, 15.27it/s, v_num=4, train_loss_step=0.0246, train_loss_epoch=0.0246]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 153.48it/s]\n",
            "Merging all model predictions...\n",
            "\n",
            "==================================================\n",
            "SUCCESS! PREDICTIONS FROM ALL MODELS:\n",
            "==================================================\n",
            "  unique_id         ds     cutoff          y      Naive  SeasonalNaive        AutoETS      AutoARIMA       LightGBM     AutoNBEATS      AutoNHITS\n",
            "0     10_72 2012-06-15 2012-06-08  105499.39  125434.23      127450.66  125453.077037  135382.011204  145607.035537  131838.000000  129803.835938\n",
            "1     10_72 2012-06-22 2012-06-08  107949.41  125434.23      117948.54  114657.507531  117358.605031  141443.112113  126753.062500  115893.484375\n",
            "2     10_72 2012-06-29 2012-06-08   96579.10  125434.23      114398.47  116075.626160  109980.247169  148534.664866  100515.156250  109323.000000\n",
            "3     10_72 2012-07-06 2012-06-08  100464.25  125434.23      108519.93  109074.835739  102381.803899  149283.028227  109166.476562  110246.250000\n",
            "4     10_72 2012-07-13 2012-07-06   92923.05  100464.25      115004.83  110645.290441  107966.684059  105233.901058  111366.523438  102473.656250\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 4. Standard Models Pipeline\n",
        "# ==========================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsforecast import StatsForecast\n",
        "from statsforecast.models import Naive, SeasonalNaive, AutoETS, AutoARIMA\n",
        "from mlforecast import MLForecast\n",
        "from mlforecast.target_transforms import Differences\n",
        "from lightgbm import LGBMRegressor\n",
        "from neuralforecast import NeuralForecast\n",
        "from neuralforecast.auto import AutoNBEATS, AutoNHITS\n",
        "\n",
        "# Set pandas to show all columns (so you can see all models)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "def run_standard_pipeline(df):\n",
        "    HORIZON = 4\n",
        "    N_WINDOWS = 5\n",
        "    FREQ = 'W-FRI'\n",
        "\n",
        "    # A. StatsForecast (Naive, ARIMA, ETS)\n",
        "    print(f\"[{'StatsForecast':<15}] Training: Naive, SeasonalNaive, AutoETS, AutoARIMA\")\n",
        "    sf = StatsForecast(\n",
        "        models=[\n",
        "            Naive(),\n",
        "            SeasonalNaive(season_length=52),\n",
        "            AutoETS(season_length=52),\n",
        "            AutoARIMA(season_length=52)\n",
        "        ],\n",
        "        freq=FREQ,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    stats_cv = sf.cross_validation(\n",
        "        df=df[['unique_id', 'ds', 'y']],\n",
        "        h=HORIZON, step_size=HORIZON, n_windows=N_WINDOWS\n",
        "    )\n",
        "\n",
        "    # B. MLForecast (LightGBM)\n",
        "    print(f\"[{'MLForecast':<15}] Training: LightGBM\")\n",
        "    lgbm = LGBMRegressor(verbosity=-1, random_state=42)\n",
        "\n",
        "    # Prepare features\n",
        "    mlforecast_cols = [\n",
        "        'unique_id', 'ds', 'y', 'Type', 'Size',\n",
        "        'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "        'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5',\n",
        "        'IsHoliday'\n",
        "    ]\n",
        "    df_for_mlforecast = df[mlforecast_cols].copy()\n",
        "\n",
        "    mlf = MLForecast(\n",
        "        models=[lgbm], freq=FREQ, lags=[1, 4, 52],\n",
        "        target_transforms=[Differences([1])],\n",
        "        date_features=['month', 'week'], num_threads=4\n",
        "    )\n",
        "\n",
        "    ml_cv = mlf.cross_validation(\n",
        "        df=df_for_mlforecast,\n",
        "        h=HORIZON, step_size=HORIZON, n_windows=N_WINDOWS,\n",
        "        static_features=['Type', 'Size']\n",
        "    )\n",
        "    ml_cv = ml_cv.rename(columns={'LGBMRegressor': 'LightGBM'})\n",
        "\n",
        "    # C. NeuralForecast (AutoNBEATS, AutoNHITS)\n",
        "    print(f\"[{'NeuralForecast':<15}] Training: AutoNBEATS, AutoNHITS\")\n",
        "\n",
        "    nf = NeuralForecast(\n",
        "        models=[\n",
        "            AutoNBEATS(h=HORIZON, num_samples=2),\n",
        "            AutoNHITS(h=HORIZON, num_samples=2)\n",
        "        ],\n",
        "        freq=FREQ\n",
        "    )\n",
        "\n",
        "    neural_cv = nf.cross_validation(\n",
        "        df=df[['unique_id', 'ds', 'y']],\n",
        "        val_size=HORIZON,\n",
        "        n_windows=N_WINDOWS,\n",
        "        step_size=HORIZON\n",
        "    )\n",
        "\n",
        "    # D. MERGING ALL RESULTS\n",
        "    print(\"Merging all model predictions...\")\n",
        "\n",
        "    # Start with StatsForecast\n",
        "    all_results = stats_cv.copy()\n",
        "\n",
        "    # Merge MLForecast (dropping 'y' to avoid duplication)\n",
        "    if ml_cv is not None:\n",
        "        all_results = all_results.merge(\n",
        "            ml_cv.drop(columns=['y'], errors='ignore'),\n",
        "            on=['unique_id', 'ds', 'cutoff'],\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "    # Merge NeuralForecast (dropping 'y' to avoid duplication)\n",
        "    if neural_cv is not None:\n",
        "        all_results = all_results.merge(\n",
        "            neural_cv.drop(columns=['y'], errors='ignore'),\n",
        "            on=['unique_id', 'ds', 'cutoff'],\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "    return all_results\n",
        "\n",
        "# EXECUTE\n",
        "if 'train_subset' in globals():\n",
        "    # Filter for active data\n",
        "    active_ids = train_subset.groupby('unique_id')['y'].sum()\n",
        "    active_ids = active_ids[active_ids > 0].index.tolist()\n",
        "\n",
        "    if len(active_ids) > 0:\n",
        "        train_subset_clean = train_subset[train_subset['unique_id'].isin(active_ids)].copy()\n",
        "\n",
        "        # Run Pipeline\n",
        "        combined_results = run_standard_pipeline(train_subset_clean)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"SUCCESS! PREDICTIONS FROM ALL MODELS:\")\n",
        "        print(\"=\"*50)\n",
        "        print(combined_results.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f5a9ec4a5dc4438fbfa016c0bbe2b381",
            "05e2d6ed75a44cd182844c4157338d13",
            "82c46e1be0ab447680d10b54c5d7330e",
            "8753ed44ab8d44ef91b20aee07d956dd",
            "419e618396394b00b1121c27211b8cd7",
            "41bd11b37cf04e1196cf4e91a80526b9",
            "02bcd606cc9b4ecba90dc9143e328a25",
            "cafa1134b4ef4b32a87348eb02475020"
          ]
        },
        "id": "ASbJMUvw7GZQ",
        "outputId": "5c603b8d-52e5-4449-f2dd-206557d8bda7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "'int' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 134\u001b[39m\n\u001b[32m    131\u001b[39m test_clean = test_subset[test_subset[\u001b[33m'\u001b[39m\u001b[33munique_id\u001b[39m\u001b[33m'\u001b[39m].isin(active_ids)].copy()\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# RUN FUNCTION\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m future_forecasts = \u001b[43mgenerate_future_forecasts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_clean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# MERGE TIMEGPT IF AVAILABLE\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mtgpt_fcst\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tgpt_fcst.empty:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 113\u001b[39m, in \u001b[36mgenerate_future_forecasts\u001b[39m\u001b[34m(df_train, df_test)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# STABILITY SETTINGS:\u001b[39;00m\n\u001b[32m     97\u001b[39m nf = NeuralForecast(\n\u001b[32m     98\u001b[39m     models=[\n\u001b[32m     99\u001b[39m         AutoNBEATS(\n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m     freq=FREQ\n\u001b[32m    111\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[43mnf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43munique_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mds\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43my\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m nf_fut = nf.predict(futr_df=df_test[[\u001b[33m'\u001b[39m\u001b[33munique_id\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mds\u001b[39m\u001b[33m'\u001b[39m]])\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# 5. Merge All Results\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\woodw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\neuralforecast\\core.py:543\u001b[39m, in \u001b[36mNeuralForecast.fit\u001b[39m\u001b[34m(self, df, static_df, val_size, use_init_models, verbose, id_col, time_col, target_col, distributed_config, prediction_intervals)\u001b[39m\n\u001b[32m    540\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset_models()\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.models):\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m     \u001b[38;5;28mself\u001b[39m.models[i] = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistributed_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdistributed_config\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[38;5;28mself\u001b[39m._fitted = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\woodw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\neuralforecast\\common\\_base_auto.py:392\u001b[39m, in \u001b[36mBaseAuto.fit\u001b[39m\u001b[34m(self, dataset, val_size, test_size, random_seed, distributed_config)\u001b[39m\n\u001b[32m    388\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m distributed_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    389\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    390\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdistributed training is not supported for the ray backend.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    391\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tune_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcls_model\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcls_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpus\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgpus\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43msearch_alg\u001b[49m\u001b[43m=\u001b[49m\u001b[43msearch_alg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m     best_config = results.get_best_result().config\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\woodw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\neuralforecast\\common\\_base_auto.py:258\u001b[39m, in \u001b[36mBaseAuto._tune_model\u001b[39m\u001b[34m(self, cls_model, dataset, val_size, test_size, cpus, gpus, verbose, num_samples, search_alg, config)\u001b[39m\n\u001b[32m    240\u001b[39m trial_dirname_creator = (\n\u001b[32m    241\u001b[39m     (\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial.trainable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial.trial_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    242\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m platform.system() == \u001b[33m\"\u001b[39m\u001b[33mWindows\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    244\u001b[39m )\n\u001b[32m    246\u001b[39m tuner = tune.Tuner(\n\u001b[32m    247\u001b[39m     tune.with_resources(train_fn_with_parameters, device_dict),\n\u001b[32m    248\u001b[39m     run_config=air.RunConfig(callbacks=\u001b[38;5;28mself\u001b[39m.callbacks, verbose=verbose),\n\u001b[32m   (...)\u001b[39m\u001b[32m    256\u001b[39m     param_space=config,\n\u001b[32m    257\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m results = \u001b[43mtuner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\woodw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\tuner.py:345\u001b[39m, in \u001b[36mTuner.fit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Executes hyperparameter tuning job as configured and returns result.\u001b[39;00m\n\u001b[32m    314\u001b[39m \n\u001b[32m    315\u001b[39m \u001b[33;03mFailure handling:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    341\u001b[39m \u001b[33;03m    RayTaskError: If user-provided trainable raises an exception\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_ray_client:\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_local_tuner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    347\u001b[39m     (\n\u001b[32m    348\u001b[39m         progress_reporter,\n\u001b[32m    349\u001b[39m         string_queue,\n\u001b[32m    350\u001b[39m     ) = \u001b[38;5;28mself\u001b[39m._prepare_remote_tuner_for_jupyter_progress_reporting()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\woodw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\impl\\tuner_internal.py:506\u001b[39m, in \u001b[36mTunerInternal.fit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    504\u001b[39m param_space = copy.deepcopy(\u001b[38;5;28mself\u001b[39m.param_space)\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_restored:\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m     analysis = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    508\u001b[39m     analysis = \u001b[38;5;28mself\u001b[39m._fit_resume(trainable, param_space)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\woodw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\impl\\tuner_internal.py:622\u001b[39m, in \u001b[36mTunerInternal._fit_internal\u001b[39m\u001b[34m(self, trainable, param_space)\u001b[39m\n\u001b[32m    609\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Fitting for a fresh Tuner.\"\"\"\u001b[39;00m\n\u001b[32m    610\u001b[39m args = {\n\u001b[32m    611\u001b[39m     **\u001b[38;5;28mself\u001b[39m._get_tune_run_arguments(trainable),\n\u001b[32m    612\u001b[39m     **\u001b[38;5;28mdict\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    620\u001b[39m     **\u001b[38;5;28mself\u001b[39m._tuner_kwargs,\n\u001b[32m    621\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m analysis = \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[38;5;28mself\u001b[39m.clear_remote_string_queue()\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m analysis\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\woodw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\tune.py:994\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001b[39m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    993\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m runner.is_finished() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment_interrupted_event.is_set():\n\u001b[32m--> \u001b[39m\u001b[32m994\u001b[39m         \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    995\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m has_verbosity(Verbosity.V1_EXPERIMENT):\n\u001b[32m    996\u001b[39m             _report_progress(runner, progress_reporter)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\woodw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\execution\\tune_controller.py:679\u001b[39m, in \u001b[36mTuneController.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    674\u001b[39m     \u001b[38;5;28mself\u001b[39m._callbacks.on_step_begin(\n\u001b[32m    675\u001b[39m         iteration=\u001b[38;5;28mself\u001b[39m._iteration, trials=\u001b[38;5;28mself\u001b[39m._trials\n\u001b[32m    676\u001b[39m     )\n\u001b[32m    678\u001b[39m \u001b[38;5;66;03m# Ask searcher for more trials\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m679\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_update_trial_queue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;66;03m# Start actors for added trials\u001b[39;00m\n\u001b[32m    682\u001b[39m \u001b[38;5;28mself\u001b[39m._maybe_add_actors()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\woodw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\execution\\tune_controller.py:779\u001b[39m, in \u001b[36mTuneController._maybe_update_trial_queue\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    774\u001b[39m dont_wait_for_trial = (\n\u001b[32m    775\u001b[39m     \u001b[38;5;28mself\u001b[39m._pending_trials \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._running_trials \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._paused_trials\n\u001b[32m    776\u001b[39m )\n\u001b[32m    778\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._pending_trials) < \u001b[38;5;28mself\u001b[39m._max_pending_trials:\n\u001b[32m--> \u001b[39m\u001b[32m779\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_trial_queue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdont_wait_for_trial\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    780\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    781\u001b[39m     dont_wait_for_trial = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\woodw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\execution\\tune_controller.py:582\u001b[39m, in \u001b[36mTuneController._update_trial_queue\u001b[39m\u001b[34m(self, blocking, timeout)\u001b[39m\n\u001b[32m    579\u001b[39m         time.sleep(\u001b[32m1\u001b[39m)\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trial:\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\woodw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\execution\\tune_controller.py:527\u001b[39m, in \u001b[36mTuneController.add_trial\u001b[39m\u001b[34m(self, trial)\u001b[39m\n\u001b[32m    524\u001b[39m \u001b[38;5;66;03m# If the config map has had all the references replaced with placeholders,\u001b[39;00m\n\u001b[32m    525\u001b[39m \u001b[38;5;66;03m# resolve them before adding the trial.\u001b[39;00m\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._placeholder_resolvers:\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m     \u001b[43mtrial\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresolve_config_placeholders\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_placeholder_resolvers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[38;5;66;03m# With trial.config resolved, create placement group factory if needed.\u001b[39;00m\n\u001b[32m    530\u001b[39m trial.create_placement_group_factory()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\woodw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\experiment\\trial.py:484\u001b[39m, in \u001b[36mTrial.resolve_config_placeholders\u001b[39m\u001b[34m(self, placeholder_resolvers)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# Make a copy of the unresolved config before resolve it.\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;28mself\u001b[39m.config = copy.deepcopy(\u001b[38;5;28mself\u001b[39m.__unresolved_config)\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m \u001b[43mresolve_placeholders\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplaceholder_resolvers\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\woodw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\impl\\placeholder.py:241\u001b[39m, in \u001b[36mresolve_placeholders\u001b[39m\u001b[34m(config, replaced)\u001b[39m\n\u001b[32m    238\u001b[39m             assign_value(config, prefix, resolver.resolve(*args))\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# RefResolvers first.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m \u001b[43m__resolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_RefResolver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# Functions need to be resolved after RefResolvers, in case they are\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# referencing values from the RefResolvers.\u001b[39;00m\n\u001b[32m    244\u001b[39m __resolve(_FunctionResolver, args=(config,))\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\woodw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\impl\\placeholder.py:235\u001b[39m, in \u001b[36mresolve_placeholders.<locals>.__resolve\u001b[39m\u001b[34m(resolver_type, args)\u001b[39m\n\u001b[32m    232\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m resolver \u001b[38;5;129;01min\u001b[39;00m resolvers:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolver.hash != \u001b[43mph\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[32m    236\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    237\u001b[39m     \u001b[38;5;66;03m# Found the matching resolver.\u001b[39;00m\n",
            "\u001b[31mTypeError\u001b[39m: 'int' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 5. Generate Testing Outputs \n",
        "# ==========================================\n",
        "import pandas as pd\n",
        "from statsforecast import StatsForecast\n",
        "from statsforecast.models import Naive, SeasonalNaive, AutoETS, AutoARIMA\n",
        "from mlforecast import MLForecast\n",
        "from mlforecast.target_transforms import Differences\n",
        "from lightgbm import LGBMRegressor\n",
        "from neuralforecast import NeuralForecast\n",
        "from neuralforecast.auto import AutoNBEATS, AutoNHITS\n",
        "from neuralforecast.losses.pytorch import MAE \n",
        "\n",
        "def generate_future_forecasts(df_train, df_test):\n",
        "    print(\"\\n--- Generating Future Forecasts (Testing Outputs) ---\")\n",
        "\n",
        "    # 1. Configuration\n",
        "    FREQ = 'W-FRI'\n",
        "    # Calculate horizon based on the test set\n",
        "    HORIZON = df_test.groupby('unique_id')['ds'].count().max()\n",
        "    print(f\"Forecasting horizon: {HORIZON} weeks\")\n",
        "\n",
        "    # 2. StatsForecast\n",
        "    print(\"1. Generating StatsForecast predictions...\")\n",
        "    sf = StatsForecast(\n",
        "        models=[\n",
        "            Naive(),\n",
        "            SeasonalNaive(season_length=52),\n",
        "            AutoETS(season_length=52),\n",
        "            AutoARIMA(season_length=52)\n",
        "        ],\n",
        "        freq=FREQ,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    sf_fut = sf.forecast(df=df_train[['unique_id', 'ds', 'y']], h=HORIZON)\n",
        "    sf_fut = sf_fut.reset_index()\n",
        "\n",
        "    # 3. MLForecast (LightGBM)\n",
        "    print(\"2. Generating MLForecast (LightGBM) predictions...\")\n",
        "    lgbm = LGBMRegressor(verbosity=-1, random_state=42)\n",
        "\n",
        "    mlf = MLForecast(\n",
        "        models=[lgbm],\n",
        "        freq=FREQ,\n",
        "        lags=[1, 4, 52],\n",
        "        target_transforms=[Differences([1])],\n",
        "        date_features=['month', 'week'],\n",
        "        num_threads=4\n",
        "    )\n",
        "\n",
        "    # Define columns\n",
        "    ml_train_cols = ['unique_id', 'ds', 'y', 'Type', 'Size', 'Temperature',\n",
        "                     'Fuel_Price', 'CPI', 'Unemployment', 'IsHoliday',\n",
        "                     'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "\n",
        "    # Fit (Defining Type and Size as static)\n",
        "    mlf.fit(df=df_train[ml_train_cols], static_features=['Type', 'Size'])\n",
        "\n",
        "    # The model already knows 'Type' and 'Size' from training.\n",
        "    cols_to_drop = ['y', 'Weekly_Sales', 'Type', 'Size']\n",
        "    X_df_future = df_test.drop(columns=cols_to_drop, errors='ignore')\n",
        "\n",
        "    ml_fut = mlf.predict(h=HORIZON, X_df=X_df_future)\n",
        "    ml_fut = ml_fut.rename(columns={'LGBMRegressor': 'LightGBM'})\n",
        "\n",
        "    # 4. NeuralForecast (Auto Models Stabilized)\n",
        "    print(\"3. Generating NeuralForecast predictions...\")\n",
        "\n",
        "    # STABILITY SETTINGS:\n",
        "    nf = NeuralForecast(\n",
        "        models=[\n",
        "            AutoNBEATS(\n",
        "                h=HORIZON,\n",
        "                loss=MAE(),\n",
        "                num_samples=5\n",
        "            ),\n",
        "            AutoNHITS(\n",
        "                h=HORIZON,\n",
        "                loss=MAE(),\n",
        "                num_samples=5\n",
        "            )\n",
        "        ],\n",
        "        freq=FREQ\n",
        "    )\n",
        "\n",
        "    nf.fit(df=df_train[['unique_id', 'ds', 'y']])\n",
        "    nf_fut = nf.predict(futr_df=df_test[['unique_id', 'ds']])\n",
        "\n",
        "    # 5. Merge All Results\n",
        "\n",
        "    print(\"Merging future forecasts...\")\n",
        "    final_fut = sf_fut.copy()\n",
        "    final_fut = final_fut.merge(ml_fut, on=['unique_id', 'ds'], how='left')\n",
        "    final_fut = final_fut.merge(nf_fut, on=['unique_id', 'ds'], how='left')\n",
        "\n",
        "    return final_fut\n",
        "\n",
        "# EXECUTE\n",
        "if 'train_subset' in globals() and 'test_subset' in globals():\n",
        "\n",
        "    # Filter for active data to ensure stability\n",
        "    active_ids = train_subset.groupby('unique_id')['y'].sum()\n",
        "    active_ids = active_ids[active_ids > 0].index.tolist()\n",
        "    train_clean = train_subset[train_subset['unique_id'].isin(active_ids)].copy()\n",
        "    test_clean = test_subset[test_subset['unique_id'].isin(active_ids)].copy()\n",
        "\n",
        "    # RUN FUNCTION\n",
        "    future_forecasts = generate_future_forecasts(train_clean, test_clean)\n",
        "\n",
        "    # Checks if TimeGPT was run in Step 3 and merges it if it exists\n",
        "    if 'tgpt_fcst' in globals() and not tgpt_fcst.empty:\n",
        "        print(\"Merging TimeGPT results...\")\n",
        "        cols_to_merge = ['unique_id', 'ds', 'TimeGPT']\n",
        "        if all(col in tgpt_fcst.columns for col in cols_to_merge):\n",
        "             future_forecasts = future_forecasts.merge(\n",
        "                 tgpt_fcst[cols_to_merge],\n",
        "                 on=['unique_id', 'ds'],\n",
        "                 how='left'\n",
        "             )\n",
        "\n",
        "    print(\"\\nSuccess! Future forecasts generated.\")\n",
        "    print(future_forecasts.head())\n",
        "\n",
        "else:\n",
        "    print(\"Error: 'train_subset' or 'test_subset' not found. Please run Step 2 (Data Loading) first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQj1yszz7aqa",
        "outputId": "6ef0b2df-ecf5-47c5-84b9-abcfa29b71ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Global Accuracy Metrics ---\n",
            "           Model           ME           MAE          RMSE      MAPE\n",
            "5     AutoNBEATS  -850.619566   7107.947170   9782.697450  5.175090\n",
            "2        AutoETS  -857.830317   7845.232619  10662.438821  5.672372\n",
            "3      AutoARIMA -2206.756063   7418.543313  11411.103296  5.473035\n",
            "6      AutoNHITS -1811.165523   9425.748159  12897.903357  6.778289\n",
            "4       LightGBM -5034.945115  10399.368105  14645.473281  7.725298\n",
            "1  SeasonalNaive  1012.752825  11298.365025  15861.602483  8.184313\n",
            "0          Naive -2025.840950  12065.408550  15934.427679  8.689676\n",
            "\n",
            "--- Model Leaderboard (Wins per Series) ---\n",
            "          Winner  count\n",
            "0     AutoNBEATS      9\n",
            "1      AutoARIMA      4\n",
            "2        AutoETS      3\n",
            "3  SeasonalNaive      3\n",
            "4      AutoNHITS      1\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 6. Evaluation Metrics\n",
        "# ==========================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "def calculate_metrics(cv_df):\n",
        "    # Identify model columns (exclude ID/Date/Target columns)\n",
        "    models = [c for c in cv_df.columns if c not in ['unique_id', 'ds', 'cutoff', 'y']]\n",
        "    results = []\n",
        "\n",
        "    for model in models:\n",
        "        y_true = cv_df['y']\n",
        "        y_pred = cv_df[model]\n",
        "\n",
        "        # Drop NaNs for metric calculation (in case of alignment issues)\n",
        "        valid_mask = ~np.isnan(y_pred)\n",
        "        y_true_clean = y_true[valid_mask]\n",
        "        y_pred_clean = y_pred[valid_mask]\n",
        "\n",
        "        if len(y_true_clean) == 0:\n",
        "            continue\n",
        "\n",
        "        # Rubric Metrics\n",
        "        me = np.mean(y_true_clean - y_pred_clean)\n",
        "        mae = mean_absolute_error(y_true_clean, y_pred_clean)\n",
        "        rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
        "\n",
        "        # MAPE (handling zeros)\n",
        "        mask = y_true_clean != 0\n",
        "        if mask.any():\n",
        "            mape = np.mean(np.abs((y_true_clean[mask] - y_pred_clean[mask]) / y_true_clean[mask])) * 100\n",
        "        else:\n",
        "            mape = np.nan\n",
        "\n",
        "        results.append({'Model': model, 'ME': me, 'MAE': mae, 'RMSE': rmse, 'MAPE': mape})\n",
        "\n",
        "    return pd.DataFrame(results).sort_values(by='RMSE')\n",
        "\n",
        "def count_winners(cv_df):\n",
        "    models = [c for c in cv_df.columns if c not in ['unique_id', 'ds', 'cutoff', 'y']]\n",
        "    errors = cv_df.copy()\n",
        "\n",
        "    # Calculate absolute error for every row\n",
        "    for m in models:\n",
        "        errors[m] = (errors['y'] - errors[m]).abs()\n",
        "\n",
        "    # Find model with lowest MAE per series\n",
        "    series_mae = errors.groupby('unique_id')[models].mean()\n",
        "    series_mae['Winner'] = series_mae.idxmin(axis=1)\n",
        "\n",
        "    return series_mae['Winner'].value_counts().reset_index()\n",
        "\n",
        "# EXECUTE THE FUNCTIONS\n",
        "# We look for 'combined_results' which comes from Step 4 (The Cross-Validation Step)\n",
        "if 'combined_results' in globals():\n",
        "    print(\"\\n--- Calculating Global Accuracy Metrics ---\")\n",
        "    metrics_df = calculate_metrics(combined_results)\n",
        "    print(metrics_df)\n",
        "\n",
        "    print(\"\\n--- Model Leaderboard (Wins per Series) ---\")\n",
        "    winners_df = count_winners(combined_results)\n",
        "    print(winners_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "V_IOb0Mr7lBq",
        "outputId": "53b6961c-875c-48cc-d6ba-9082c0c1a1a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Required functions are missing.\n",
            "Please make sure you have run ALL previous cells (Steps 2-6) before running this one.\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 7. Main Execution \n",
        "# ==========================================\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Check if functions from previous cells are defined\n",
        "required_funcs = ['load_and_prep_data', 'run_standard_pipeline', 'run_timegpt_pipeline',\n",
        "                  'generate_future_forecasts', 'calculate_metrics', 'count_winners']\n",
        "\n",
        "if not all(func in globals() for func in required_funcs):\n",
        "    print(\"Error: Required functions are missing.\")\n",
        "    print(\"Please make sure you have run ALL previous cells (Steps 2-6) before running this one.\")\n",
        "else:\n",
        "    print(\"Starting Main Execution Pipeline...\\n\")\n",
        "\n",
        "    # 1. Load Data\n",
        "    train_subset, test_subset = load_and_prep_data()\n",
        "\n",
        "    if train_subset is not None:\n",
        "        # 2. Run Standard Model\n",
        "        final_eval = run_standard_pipeline(train_subset)\n",
        "\n",
        "        # 3. Run TimeGPT (Remote)\n",
        "        tgpt_cv, tgpt_fcst = run_timegpt_pipeline(train_subset, test_subset, horizon=4, n_windows=5)\n",
        "\n",
        "        # 4. Merge TimeGPT into Evaluation Results\n",
        "        print(\"Merging TimeGPT results...\")\n",
        "        merge_keys = ['unique_id', 'ds', 'cutoff']\n",
        "\n",
        "        if not tgpt_cv.empty and 'TimeGPT' in tgpt_cv.columns:\n",
        "            # Drop 'y' from TimeGPT CV to avoid duplicates if it exists\n",
        "            tgpt_clean = tgpt_cv.drop(columns=['y'], errors='ignore')\n",
        "            final_eval = final_eval.merge(tgpt_clean, on=merge_keys, how='left')\n",
        "\n",
        "        # 5. Generate Future Forecasts\n",
        "        future_forecasts = generate_future_forecasts(train_subset, test_subset)\n",
        "\n",
        "        # Merge TimeGPT Future Forecasts if available\n",
        "        if not tgpt_fcst.empty and 'TimeGPT' in tgpt_fcst.columns:\n",
        "             future_forecasts = future_forecasts.merge(\n",
        "                 tgpt_fcst[['unique_id', 'ds', 'TimeGPT']],\n",
        "                 on=['unique_id', 'ds'],\n",
        "                 how='left'\n",
        "             )\n",
        "\n",
        "        # 6. Metrics & Winners\n",
        "        metrics_df = calculate_metrics(final_eval)\n",
        "        winners_df = count_winners(final_eval)\n",
        "\n",
        "        print(\"\\n--- Final Metrics ---\")\n",
        "        print(metrics_df)\n",
        "        print(\"\\n--- Model Winners (Count by Series) ---\")\n",
        "        print(winners_df)\n",
        "\n",
        "        # 7. Save CSVs\n",
        "        final_eval.to_csv('final_evaluation_output.csv', index=False)\n",
        "        metrics_df.to_csv('final_metrics_summary.csv', index=False)\n",
        "        future_forecasts.to_csv('testing_outputs.csv', index=False)\n",
        "        print(\"\\n Files Saved: final_evaluation_output.csv, final_metrics_summary.csv, testing_outputs.csv\")\n",
        "\n",
        "        # 8. Plotting\n",
        "        # Check if we have data to plot\n",
        "        if not final_eval.empty:\n",
        "            u_id = final_eval['unique_id'].unique()[0]\n",
        "            subset = final_eval[final_eval['unique_id'] == u_id]\n",
        "\n",
        "            plt.figure(figsize=(14, 6))\n",
        "\n",
        "            # Plot Actuals\n",
        "            if 'y' in subset.columns:\n",
        "                plt.plot(subset['ds'], subset['y'], label='Actual', color='black', linewidth=2)\n",
        "\n",
        "            # Plot models dynamically\n",
        "            plot_models = [c for c in subset.columns if c not in ['unique_id', 'ds', 'cutoff', 'y']]\n",
        "            for m in plot_models:\n",
        "                # Plot only if column is numeric\n",
        "                if pd.api.types.is_numeric_dtype(subset[m]):\n",
        "                    plt.plot(subset['ds'], subset[m], label=m, alpha=0.7)\n",
        "\n",
        "            plt.title(f\"Forecast Models vs Actual: {u_id}\")\n",
        "            plt.legend()\n",
        "            plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02bcd606cc9b4ecba90dc9143e328a25": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_cafa1134b4ef4b32a87348eb02475020",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Predicting <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> 1/1 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:00 • 0:00:00</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">0.00it/s</span>  \n</pre>\n",
                  "text/plain": "Predicting \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 1/1 \u001b[2m0:00:00 • 0:00:00\u001b[0m \u001b[2;4m0.00it/s\u001b[0m  \n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "05e2d6ed75a44cd182844c4157338d13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "165bba3b67174d3a8e9cf02d0420a0d3": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_776a083e13504f6b80bbf0908cec4d40",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Predicting <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> 1/1 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:00 • 0:00:00</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">0.00it/s</span>  \n</pre>\n",
                  "text/plain": "Predicting \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 1/1 \u001b[2m0:00:00 • 0:00:00\u001b[0m \u001b[2;4m0.00it/s\u001b[0m  \n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "2209404beb6840a79cf0e4ca1640eadf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bde3429839241c78b56644d387f00d8": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_cc25f337b0c0499baa683f80fcaab583",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 999/-2 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> 1/1 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:00 • 0:00:00</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">0.00it/s</span> <span style=\"font-style: italic\">v_num: 11.000 train_loss_step: 0.086</span>\n                                                                               <span style=\"font-style: italic\">train_loss_epoch: 0.104             </span>\n</pre>\n",
                  "text/plain": "Epoch 999/-2 \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 1/1 \u001b[2m0:00:00 • 0:00:00\u001b[0m \u001b[2;4m0.00it/s\u001b[0m \u001b[3mv_num: 11.000 train_loss_step: 0.086\u001b[0m\n                                                                               \u001b[3mtrain_loss_epoch: 0.104             \u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "419e618396394b00b1121c27211b8cd7": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_41bd11b37cf04e1196cf4e91a80526b9",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Predicting <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> 1/1 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:00 • 0:00:00</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">0.00it/s</span>  \n</pre>\n",
                  "text/plain": "Predicting \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 1/1 \u001b[2m0:00:00 • 0:00:00\u001b[0m \u001b[2;4m0.00it/s\u001b[0m  \n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "41bd11b37cf04e1196cf4e91a80526b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "776a083e13504f6b80bbf0908cec4d40": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79a1e0e5a458433985cc96d6332abc7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82c46e1be0ab447680d10b54c5d7330e": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_8753ed44ab8d44ef91b20aee07d956dd",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 799/-2 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> 1/1 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:00 • 0:00:00</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">0.00it/s</span> <span style=\"font-style: italic\">v_num: 16.000 train_loss_step:      </span>\n                                                                               <span style=\"font-style: italic\">1335.244 train_loss_epoch: 1343.614 </span>\n</pre>\n",
                  "text/plain": "Epoch 799/-2 \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 1/1 \u001b[2m0:00:00 • 0:00:00\u001b[0m \u001b[2;4m0.00it/s\u001b[0m \u001b[3mv_num: 16.000 train_loss_step:      \u001b[0m\n                                                                               \u001b[3m1335.244 train_loss_epoch: 1343.614 \u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "8753ed44ab8d44ef91b20aee07d956dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a112c8827a5d4fbca42d0e7eed3c73c7": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_2209404beb6840a79cf0e4ca1640eadf",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Predicting <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> 1/1 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:00 • 0:00:00</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">0.00it/s</span>  \n</pre>\n",
                  "text/plain": "Predicting \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 1/1 \u001b[2m0:00:00 • 0:00:00\u001b[0m \u001b[2;4m0.00it/s\u001b[0m  \n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "cafa1134b4ef4b32a87348eb02475020": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc25f337b0c0499baa683f80fcaab583": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5a9ec4a5dc4438fbfa016c0bbe2b381": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_05e2d6ed75a44cd182844c4157338d13",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 999/-2 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> 1/1 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:00 • 0:00:00</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">0.00it/s</span> <span style=\"font-style: italic\">v_num: 15.000 train_loss_step: 0.072</span>\n                                                                               <span style=\"font-style: italic\">train_loss_epoch: 0.072             </span>\n</pre>\n",
                  "text/plain": "Epoch 999/-2 \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 1/1 \u001b[2m0:00:00 • 0:00:00\u001b[0m \u001b[2;4m0.00it/s\u001b[0m \u001b[3mv_num: 15.000 train_loss_step: 0.072\u001b[0m\n                                                                               \u001b[3mtrain_loss_epoch: 0.072             \u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "fbc59f91b6df49cc9163c5cc6b3be16e": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_79a1e0e5a458433985cc96d6332abc7c",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 499/-2 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> 1/1 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:00 • 0:00:00</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">0.00it/s</span> <span style=\"font-style: italic\">v_num: 13.000 train_loss_step: 0.024</span>\n                                                                               <span style=\"font-style: italic\">train_loss_epoch: 0.029             </span>\n</pre>\n",
                  "text/plain": "Epoch 499/-2 \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 1/1 \u001b[2m0:00:00 • 0:00:00\u001b[0m \u001b[2;4m0.00it/s\u001b[0m \u001b[3mv_num: 13.000 train_loss_step: 0.024\u001b[0m\n                                                                               \u001b[3mtrain_loss_epoch: 0.029             \u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
